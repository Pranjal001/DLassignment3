{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac9d613",
      "metadata": {
        "id": "8ac9d613"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import copy\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import heapq\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f99e297",
      "metadata": {
        "id": "8f99e297"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e6838a",
      "metadata": {
        "id": "22e6838a"
      },
      "outputs": [],
      "source": [
        "class lang:\n",
        "    def __init__(self,path_train,path_val,path_test):\n",
        "        self.path_train = path_train\n",
        "        self.path_val = path_val\n",
        "        self.path_test = path_test\n",
        "        self.trainfile = pd.read_csv(path_train,header=None, encoding='utf-8')\n",
        "        self.valfile = pd.read_csv(path_val,header=None, encoding='utf-8')\n",
        "        self.testfile = pd.read_csv(path_test,header=None, encoding='utf-8')\n",
        "\n",
        "    def datasetencoder(self,file):\n",
        "\n",
        "        # Update file[0]\n",
        "        file[0] = [x + '>' for x in file[0]]\n",
        "\n",
        "        # Update file[1]\n",
        "        file[1] = ['<' + x + '>' for x in file[1]]\n",
        "\n",
        "        # Calculate maximum length of unique elements in file[0]\n",
        "        ipmax = 0\n",
        "        for x in file[0].unique():\n",
        "            if len(x) > ipmax:\n",
        "                ipmax = len(x)\n",
        "\n",
        "        # Calculate maximum length of unique elements in file[1]\n",
        "        opmax = 0\n",
        "        for x in file[1].unique():\n",
        "            if len(x) > opmax:\n",
        "                opmax = len(x)\n",
        "\n",
        "\n",
        "        return ipmax,opmax,file\n",
        "\n",
        "    def dictionary_create(self,data):\n",
        "\n",
        "        data.discard('<')\n",
        "        data.discard('>')\n",
        "        chartoint = {\"\": 0, '<':1, '>':2}\n",
        "        inttochar = {}\n",
        "\n",
        "        for ci, c in enumerate(sorted(data), len(chartoint)):\n",
        "            chartoint[c] = ci\n",
        "        for c, ci in chartoint.items():\n",
        "            inttochar[ci] = c\n",
        "\n",
        "        return chartoint,inttochar\n",
        "\n",
        "    def convert_tensor_element(self,data , length , chartoint):\n",
        "\n",
        "        data_enc = np.zeros(length)\n",
        "        encoder = []\n",
        "        for char in data:\n",
        "            encoder.append(chartoint[char])\n",
        "        encoder = np.array(encoder)\n",
        "\n",
        "        if len(encoder) < length:\n",
        "          length = len(encoder)\n",
        "\n",
        "        data_enc[:length] = encoder[:length]\n",
        "\n",
        "        return torch.tensor(data_enc, dtype=torch.int64)\n",
        "\n",
        "    def convert_tensor_data(self,data,maxlength_ip, chartoint_ip,maxlength_op, chartoint_op):\n",
        "\n",
        "        tensor_obj_input = []\n",
        "        tensor_obj_output = []\n",
        "\n",
        "        for ip, op in zip(data[0], data[1]):\n",
        "\n",
        "            temp_input,temp_output = self.convert_tensor_element(ip, maxlength_ip, chartoint_ip),self.convert_tensor_element(op, maxlength_op, chartoint_op)\n",
        "            tensor_obj_output.append(temp_output)\n",
        "            tensor_obj_input.append(temp_input)\n",
        "\n",
        "        tensor_obj_input,tensor_obj_output =  torch.stack(tensor_obj_input),torch.stack(tensor_obj_output)\n",
        "\n",
        "\n",
        "        return tensor_obj_input , tensor_obj_output\n",
        "\n",
        "    def preparedata(self):\n",
        "\n",
        "        train_ipmax , train_opmax , train = self.datasetencoder(self.trainfile)\n",
        "        val_ipmax , val_opmax , val =self.datasetencoder(self.valfile)\n",
        "        test_ipmax , test_opmax , test =self.datasetencoder(self.testfile)\n",
        "\n",
        "        input_char_to_int,input_int_to_char  = self.dictionary_create(set(''.join(train[0]) + ''.join(val[0]) + ''.join(test[0])))\n",
        "        output_char_to_int ,output_int_to_char= self.dictionary_create(set(''.join(train[1]) + ''.join(val[1]) + ''.join(test[1])))\n",
        "\n",
        "        ipmax = max(train_ipmax ,val_ipmax ,test_ipmax)\n",
        "        opmax = max(train_opmax ,val_opmax , test_opmax)\n",
        "\n",
        "        train_tensor_ip, train_tensor_op = self.convert_tensor_data(train,ipmax,input_char_to_int,opmax,output_char_to_int)\n",
        "        val_tensor_ip, val_tensor_op = self.convert_tensor_data(val,ipmax,input_char_to_int,opmax,output_char_to_int)\n",
        "        test_tensor_ip, test_tensor_op = self.convert_tensor_data(test,ipmax,input_char_to_int,opmax,output_char_to_int)\n",
        "\n",
        "        #transpose data tensor\n",
        "\n",
        "        train_tensor_ip, train_tensor_op = train_tensor_ip.t(),train_tensor_op.t()\n",
        "        val_tensor_ip, val_tensor_op = val_tensor_ip.t(), val_tensor_op.t()\n",
        "        test_tensor_ip, test_tensor_op = test_tensor_ip.t() , test_tensor_op.t()\n",
        "\n",
        "        len_max = max(ipmax,opmax)\n",
        "\n",
        "        return train_tensor_ip, train_tensor_op ,val_tensor_ip, val_tensor_op,test_tensor_ip, test_tensor_op,input_char_to_int,input_int_to_char,output_char_to_int ,output_int_to_char,val,len_max\n",
        "\n",
        "        #do all func call in this and return final datasets\n",
        "\n",
        "\n",
        "# pathtrain = \"/kaggle/input/akshatantra/aksharantar_sampled/hin/hin_train.csv\"\n",
        "# pathval = \"/kaggle/input/akshatantra/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "# pathtest = \"/kaggle/input/akshatantra/aksharantar_sampled/hin/hin_test.csv\"\n",
        "\n",
        "# language = lang(pathtrain , pathval, pathtest)\n",
        "# train_ip, train_op ,val_ip, val_op,test_ip, test_op,input_char_to_int,input_int_to_char,output_char_to_int ,output_int_to_char,df_val,len_max = language.preparedata()\n",
        "\n",
        "# print(val_ip.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a77bee6",
      "metadata": {
        "id": "7a77bee6"
      },
      "source": [
        "## Modules for encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ffcdb8",
      "metadata": {
        "id": "e7ffcdb8"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "\n",
        "        attention_score = torch.sum(hidden * encoder_outputs, dim=2)\n",
        "        attention_score = attention_score.t()\n",
        "        attention_wt = F.softmax(attention_score, dim=1)\n",
        "        return attention_wt.unsqueeze(1)\n",
        "\n",
        "class EncoderModule(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, layers, dropout, bidirectional, module_type):\n",
        "\n",
        "\n",
        "        super(EncoderModule, self).__init__()\n",
        "        self.mng = -1\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.count = 0\n",
        "        self.module_dict = [\"GRU\" , \"LSTM\" , \"RNN\"]\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.layers = layers\n",
        "        self.module_type = module_type\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if module_type == self.module_dict[0]:\n",
        "            self.rnn = nn.GRU(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        if module_type == self.module_dict[2]:\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        if module_type == self.module_dict[1]:\n",
        "            self.rnn = nn.LSTM(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "        if self.module_type == \"GRU\":\n",
        "\n",
        "            outputs, hidden = self.rnn(embedding)\n",
        "            if self.bidirectional == True:\n",
        "\n",
        "                outputs,hidden = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:] , torch.cat((hidden[: self.layers], hidden[self.layers:]), dim=0)\n",
        "\n",
        "\n",
        "\n",
        "            return outputs, hidden\n",
        "        if self.module_type == \"LSTM\":\n",
        "            # Pass through the LSTM layer\n",
        "            outputs, (hidden, cell) = self.rnn(embedding) # outputs shape: (seq_length, N, hidden_size)\n",
        "            if self.bidirectional == True:\n",
        "                # Sum the bidirectional outputs\n",
        "              outputs,hidden = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:] , torch.cat((hidden[: self.layers], hidden[self.layers:]), dim=0)\n",
        "\n",
        "            return outputs, hidden, cell\n",
        "\n",
        "            # Return hidden state and cell state\n",
        "\n",
        "        if self.module_type == \"RNN\":\n",
        "            # Pass through the RNN/GRU layer\n",
        "            outputs, hidden = self.rnn(embedding) # outputs shape: (seq_length, N, hidden_size)\n",
        "            if self.bidirectional == True:\n",
        "                # Sum the bidirectional outputs\n",
        "              outputs,hidden = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:] , torch.cat((hidden[: self.layers], hidden[self.layers:]), dim=0)\n",
        "\n",
        "            # Return output (max_seq, N, hidden size)\n",
        "            return outputs, hidden\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DecoderModule(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, layers, dropout, bidirection_type, cell_type):\n",
        "        super(DecoderModule, self).__init__()\n",
        "        self.mng = -1\n",
        "        self.counttemp = 0\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.module_dict = [\"GRU\" , \"LSTM\" , \"RNN\"]\n",
        "        self.bidirectional = bidirection_type\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cell_type = cell_type\n",
        "        self.embedding_size = embedding_size\n",
        "        self.layers = layers\n",
        "\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "        if module_type == self.module_dict[0]:\n",
        "            self.rnn = nn.GRU(embedding_size, hidden_size, layers, dropout=dropout)\n",
        "        if module_type == self.module_dict[1]:\n",
        "            self.rnn = nn.LSTM(embedding_size, hidden_size, layers, dropout=dropout)\n",
        "        if module_type == self.module_dict[2]:\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, layers, dropout=dropout)\n",
        "\n",
        "\n",
        "\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        self.attn = Attention(hidden_size)\n",
        "\n",
        "    def forward(self, x, encoder_outputs, hidden, cell):\n",
        "\n",
        "        embedding = self.embedding(x.unsqueeze(0))\n",
        "        embedding = self.dropout(embedding)\n",
        "\n",
        "\n",
        "        if self.cell_type == \"GRU\" :\n",
        "\n",
        "            outputs, hidden = self.rnn(embedding, hidden)\n",
        "\n",
        "\n",
        "            attention_weights = self.attn(outputs, encoder_outputs)\n",
        "            encoder_outputs=encoder_outputs.transpose(0, 1)\n",
        "            context = attention_weights.bmm(encoder_outputs)\n",
        "\n",
        "            concat_input = torch.cat((outputs.squeeze(0), context.squeeze(1)), 1)\n",
        "            concat_input = self.concat(concat_input)\n",
        "            concat_output = torch.tanh(concat_input)\n",
        "\n",
        "\n",
        "\n",
        "            out = self.fc(concat_output)\n",
        "            predictions = self.log_softmax(out)\n",
        "\n",
        "            return predictions, hidden,attention_weights.squeeze(1)\n",
        "\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "\n",
        "\n",
        "            attention_weights = self.attn(outputs, encoder_outputs)\n",
        "            encoder_outputs = encoder_outputs.transpose(0, 1)\n",
        "            context =attention_weights.bmm(encoder_outputs)\n",
        "            concat_input = torch.cat((outputs.squeeze(0), context.squeeze(1)), 1)\n",
        "            concat_input = self.concat(concat_input)\n",
        "            concat_output = torch.tanh(concat_input)\n",
        "\n",
        "\n",
        "            out = self.fc(concat_output)\n",
        "            predictions = self.log_softmax(out)\n",
        "\n",
        "            return predictions, hidden, cell, attention_weights.squeeze(1)\n",
        "\n",
        "        if self.cell_type == \"RNN\":\n",
        "\n",
        "            outputs, hidden = self.rnn(embedding, hidden)\n",
        "\n",
        "\n",
        "            attention_weights = self.attn(outputs, encoder_outputs)\n",
        "            encoder_outputs=encoder_outputs.transpose(0, 1)\n",
        "            context = attention_weights.bmm(encoder_outputs)\n",
        "\n",
        "            concat_input = self.concat(concat_input)\n",
        "            concat_output = torch.tanh(concat_input)\n",
        "\n",
        "\n",
        "\n",
        "            out = self.fc(concat_output)\n",
        "            predictions = self.log_softmax(out)\n",
        "\n",
        "            return predictions, hidden,attention_weights.squeeze(1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97aa96f9",
      "metadata": {
        "id": "97aa96f9"
      },
      "source": [
        "## Seq2Seq Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a2ab9f",
      "metadata": {
        "id": "05a2ab9f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, output_char_to_int, teacher_forcing, module_type):\n",
        "\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.mng = -1\n",
        "        self.counttemp = 0\n",
        "        self.teacher_force_ratio = teacher_forcing\n",
        "        self.module_type = module_type\n",
        "        self.target_vocab_size = len(output_char_to_int)\n",
        "\n",
        "    def forward(self, source, target):\n",
        "\n",
        "\n",
        "        target_vocab_size = self.target_vocab_size\n",
        "        teacher_force_ratio = self.teacher_force_ratio\n",
        "        batch_size = source.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "        outputs = torch.zeros(target.shape[0], batch_size, target_vocab_size).to(source.device)\n",
        "\n",
        "        x = target[0,:]\n",
        "\n",
        "        if self.module_type == 'GRU':\n",
        "            encoder_outputs, hidden = self.encoder(source)\n",
        "            hidden =  hidden[:self.decoder.layers]\n",
        "        if self.module_type == 'RNN':\n",
        "            encoder_outputs, hidden = self.encoder(source)\n",
        "            hidden =  hidden[:self.decoder.layers]\n",
        "        if self.module_type == 'LSTM':\n",
        "            encoder_outputs, hidden, cell = self.encoder(source)\n",
        "            hidden,cell =  hidden[:self.decoder.layers],cell[:self.decoder.layers]\n",
        "\n",
        "        for t in range(1, target.shape[0]):\n",
        "\n",
        "            if self.module_type == 'LSTM':\n",
        "                output, hidden, cell, _ = self.decoder(x, encoder_outputs, hidden, cell)\n",
        "            if self.module_type == 'RNN':\n",
        "                output, hidden, _ = self.decoder(x, encoder_outputs, hidden, None)\n",
        "            if self.module_type == 'GRU':\n",
        "                output, hidden, _ = self.decoder(x, encoder_outputs, hidden, None)\n",
        "\n",
        "\n",
        "\n",
        "            outputs[t] =  output\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            if random.random() >= teacher_force_ratio:\n",
        "              x = best_guess\n",
        "            else:\n",
        "              x = target[t]\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958e4d20",
      "metadata": {
        "id": "958e4d20"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd523c22",
      "metadata": {
        "id": "dd523c22"
      },
      "outputs": [],
      "source": [
        "def beam_search(model, input_seq, max_length, input_char_index, output_char_index, reverse_target_char_index, beam_width, length_penalty, cell_type):\n",
        "\n",
        "    lengthip = len(input_seq)\n",
        "    check = False\n",
        "    if lengthip > max_length:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "    input_data = np.zeros((max_length, 1), dtype=int) # (N,1)\n",
        "    count = 0\n",
        "\n",
        "    # Encode the input\n",
        "    for idx, char in enumerate(input_seq):\n",
        "        input_data[idx, 0] = input_char_index[char]\n",
        "        count += 1\n",
        "    input_data[idx + 1, 0] = input_char_index[\">\"]\n",
        "\n",
        "    # Convert to tensor\n",
        "    if(count < 0 ):\n",
        "      input_tensor = torch.tensor(input_data, dtype=torch.int64).to(device)\n",
        "    else :\n",
        "      input_tensor = torch.tensor(input_data, dtype=torch.int64).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        if cell_type == 'GRU':\n",
        "            encoder_outputs, hidden = model.encoder(input_tensor)\n",
        "            check = True\n",
        "            hidden =  hidden[:model.decoder.layers]\n",
        "        if cell_type == 'RNN':\n",
        "            encoder_outputs, hidden = model.encoder(input_tensor)\n",
        "            check = True\n",
        "            hidden =  hidden[:model.decoder.layers]\n",
        "        if cell_type == 'LSTM':\n",
        "            encoder_outputs, hidden, cell = model.encoder(input_tensor)\n",
        "            check = True\n",
        "            hidden,cell =  hidden[:model.decoder.layers],cell[:model.decoder.layers]\n",
        "\n",
        "    # Initialize beam\n",
        "    out_t = output_char_index['<']\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    initial_sequence = torch.tensor(out_reshape).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden.unsqueeze(0))]\n",
        "    lcn = len(output_char_index)\n",
        "    for _ in range(lcn):\n",
        "        candidates = []\n",
        "        counttem = 1\n",
        "        for score, seq, hidden in beam:\n",
        "\n",
        "            if  output_char_index['>'] == seq[-1].item():\n",
        "\n",
        "                counttem = 0\n",
        "                candidates.append((score, seq, hidden))\n",
        "                continue\n",
        "\n",
        "            last_token = np.array(seq[-1].item())\n",
        "            x = torch.tensor(last_token.reshape(1,)).to(device)\n",
        "\n",
        "            if cell_type == 'LSTM':\n",
        "                output, hidden, cell, _ = model.decoder(x, encoder_outputs, hidden.squeeze(0), cell)\n",
        "            if cell_type == 'GRU':\n",
        "                output, hidden, _ = model.decoder(x, encoder_outputs, hidden.squeeze(0), None)\n",
        "            if cell_type == 'RNN':\n",
        "                output, hidden, _ = model.decoder(x, encoder_outputs, hidden.squeeze(0), None)\n",
        "\n",
        "\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n",
        "            itrtopk = topk_probs[0]\n",
        "            itrtoktokens =  topk_tokens[0]\n",
        "            for prob, token in zip(itrtopk, itrtoktokens):\n",
        "                tokentemp = token.unsqueeze(0)\n",
        "                new_seq = torch.cat((seq, tokentemp), dim=0)\n",
        "                length_newsq = len(new_seq)\n",
        "                seq_length_norm_factor = (length_newsq  - 1)\n",
        "                seq_length_norm_factor = seq_length_norm_factor/5\n",
        "                candidate_score_deno =  (seq_length_norm_factor ** length_penalty)\n",
        "                candidate_score = score + torch.log(prob).item() / candidate_score_deno\n",
        "                candidates.append((candidate_score, new_seq, hidden.unsqueeze(0)))\n",
        "\n",
        "        keyq = lambda x: x[0]\n",
        "        beam = heapq.nlargest(beam_width, candidates, key = keyq)\n",
        "\n",
        "    best_score = float('-inf')\n",
        "    best_sequence = None\n",
        "\n",
        "    for score, sequence, _ in beam:\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_sequence = sequence\n",
        "\n",
        "    result = []\n",
        "    for token in best_sequence[1:]:\n",
        "        char = reverse_target_char_index[token.item()]\n",
        "        result.append(char)\n",
        "\n",
        "    final_string = ''.join(result)\n",
        "\n",
        "    return final_string\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TRAINING FUNCTION\n",
        "def train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length, wandb_log):\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_words = 0\n",
        "        accuracy = 0\n",
        "        correct_pred = 0\n",
        "        model.train()\n",
        "\n",
        "\n",
        "        # train_data_iterator =\n",
        "\n",
        "        for (x, y) in tqdm(zip(train_batch_x, train_batch_y), total=len(train_batch_x)):\n",
        "\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x.to(device), y.to(device))\n",
        "\n",
        "            target = y.to(device)\n",
        "            target = target.reshape(-1)\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            loss = criterion(output[(target != 0)], target[(target != 0)])\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            total_words = total_words + target.size(0)\n",
        "            checkq = torch.argmax(output, dim=1) == target\n",
        "            correct_pred = correct_pred + torch.sum(checkq).item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_batch_x)\n",
        "        acc = correct_pred / total_words\n",
        "        acc *= 100\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_total_loss = 0\n",
        "            val_total_words = 0\n",
        "            val_correct_pred = 0\n",
        "\n",
        "\n",
        "            # val_data_iterator =\n",
        "\n",
        "            for x_val, y_val in tqdm(zip(val_batch_x, val_batch_y),total =len(val_batch_x)):\n",
        "                target_val = y_val.to(device)\n",
        "                inp_data_val = x_val.to(device)\n",
        "\n",
        "                output_val = model(inp_data_val, target_val)\n",
        "\n",
        "\n",
        "                target_val = target_val.reshape(-1)\n",
        "                opvalshape = output_val.shape[2]\n",
        "                output_val = output_val.reshape(-1, opvalshape)\n",
        "\n",
        "                pad_mask = (target_val != 0)\n",
        "                target_val = target_val[pad_mask]\n",
        "                output_val = output_val[pad_mask]\n",
        "\n",
        "                val_loss = criterion(output_val, target_val)\n",
        "                val_total_loss = val_total_loss+ val_loss.item()\n",
        "                val_total_words = val_total_words+ target_val.size(0)\n",
        "                checkq = torch.argmax(output_val, dim=1) == target_val\n",
        "                val_correct_pred = val_correct_pred+ torch.sum(checkq).item()\n",
        "\n",
        "\n",
        "            val_acc = val_correct_pred / val_total_words\n",
        "            val_acc = 100*val_acc\n",
        "            val_avg_loss = val_total_loss / len(val_batch_x)\n",
        "\n",
        "\n",
        "        beam_val_pred = 0\n",
        "        beam_val = 0\n",
        "        itrtqdm = tqdm(range(df_val.shape[0]))\n",
        "        for i in itrtqdm:\n",
        "            input_seq,true_seq  = df_val.iloc[i, 0][:-1],df_val.iloc[i, 1][1:-1]\n",
        "            predicted_output = beam_search(model, input_seq, max_length, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type)\n",
        "            cod = true_seq == predicted_output[:-1]\n",
        "            if cod:\n",
        "                beam_val_pred = beam_val_pred + 1\n",
        "        beam_val = beam_val_pred/df_val.shape[0]\n",
        "        beam_val = 100*beam_val\n",
        "\n",
        "        print(\"========================================================================\")\n",
        "        print(f\"---------------------------- Epoch : \",epoch+1,\"------------------------\")\n",
        "        print(f\"Train accuracy Character: \",acc)\n",
        "        print(f\"Train Average Loss: \",avg_loss)\n",
        "        print(f\"Validation accuracy Character: \",val_acc)\n",
        "        print(f\"Validation Average Loss: \",val_avg_loss)\n",
        "        print(f\"Beam Val Word accuracy: \" ,beam_val)\n",
        "        print(f\"Correct Prediction : {beam_val_pred}/{df_val.shape[0]}\")\n",
        "        print(\"========================================================================\")\n",
        "\n",
        "        # if wandb_log == 1:\n",
        "        #     wandb.log({\n",
        "        #         \"train_accuracy_char\": accuracy,\n",
        "        #         \"train_loss\": avg_loss,\n",
        "        #         \"val_acc_char\": val_acc,\n",
        "        #         \"val_loss\": val_avg_loss,\n",
        "        #         \"beam_val_acc_word\" : beam_val,\n",
        "        #     })\n",
        "\n",
        "\n",
        "    return model, beam_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YiJhWZu95BtM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiJhWZu95BtM",
        "outputId": "adc05089-4641-4b21-d9c8-2bd33a96a41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): EncoderModule(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (embedding): Embedding(29, 64)\n",
            "    (rnn): GRU(64, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): DecoderModule(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (embedding): Embedding(68, 64)\n",
            "    (rnn): GRU(64, 512, num_layers=2, dropout=0.3)\n",
            "    (concat): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (fc): Linear(in_features=512, out_features=68, bias=True)\n",
            "    (log_softmax): LogSoftmax(dim=1)\n",
            "    (attn): Attention()\n",
            "  )\n",
            ")\n",
            "Total Trainable Parameters: 9529988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:49<00:00, 16.22it/s]\n",
            "100%|██████████| 64/64 [00:01<00:00, 58.40it/s]\n",
            "100%|██████████| 4096/4096 [00:47<00:00, 85.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "---------------------------- Epoch :  1 ------------------------\n",
            "Train accuracy Character:  20.0126953125\n",
            "Train Average Loss:  2.085016617923975\n",
            "Validation accuracy Character:  63.319051699514425\n",
            "Validation Average Loss:  1.3349453192204237\n",
            "Beam Val Word accuracy:  20.166015625\n",
            "Correct Prediction : 826/4096\n",
            "========================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def assign_opt(optimizer):\n",
        "  if optimizer == 'adam':\n",
        "      return optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  if optimizer == 'sgd':\n",
        "      return optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  if optimizer == 'rmsprop':\n",
        "      return optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "  if optimizer == 'nadam':\n",
        "      return optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  if optimizer == 'adagrad':\n",
        "      return optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "pathtrain = \"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/hin/hin_train.csv\"\n",
        "pathval = \"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "pathtest = \"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/hin/hin_test.csv\"\n",
        "language = lang(pathtrain , pathval, pathtest)\n",
        "train_ip, train_op ,val_ip, val_op,test_ip, test_op,input_char_to_int,input_int_to_char,output_char_to_int ,output_int_to_char,df_val,max_length = language.preparedata()\n",
        "\n",
        "# df_train, train_input_len, train_out_len = load_dataset(pathtrain)\n",
        "# df_val, val_input_len, val_out_len = load_dataset(pathval)\n",
        "# df_test, test_input_len, test_out_len = load_dataset(pathtest)\n",
        "\n",
        "# input_max_len = max(train_input_len, val_input_len, test_input_len)\n",
        "# output_max_len = max(train_out_len, val_out_len, test_out_len)\n",
        "\n",
        "\n",
        "# # Create Look Up Table\n",
        "# input_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\n",
        "# output_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n",
        "\n",
        "# # print(\"Input Lookup Table:\", input_char_to_int)\n",
        "# # print(\"\\n\\n Output Lookup Table\", output_char_to_int)\n",
        "\n",
        "# # Data Embedding and Converting them into Tensor\n",
        "# train_inputs, train_outputs = get_tensor_object(df_train, input_max_len, input_max_len, input_char_to_int, output_char_to_int)\n",
        "# val_inputs, val_outputs = get_tensor_object(df_val, input_max_len, input_max_len, input_char_to_int, output_char_to_int)\n",
        "# test_inputs, test_outputs = get_tensor_object(df_test, input_max_len, input_max_len, input_char_to_int, output_char_to_int)\n",
        "\n",
        "# # Transpose column wise\n",
        "# train_ip, train_op = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\n",
        "# val_ip, val_op = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\n",
        "# test_ip, test_op = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n",
        "\n",
        "# max_length = max(input_max_len, output_max_len)\n",
        "\n",
        "# Initialize Hyperparameters\n",
        "input_size = len(input_char_to_int)\n",
        "output_size = len(output_char_to_int)\n",
        "embedding_size = 64\n",
        "hidden_size = 512\n",
        "enc_layers = 2\n",
        "dec_layers = 2\n",
        "module_type = \"GRU\"\n",
        "dropout = 0.3\n",
        "learning_rate = 0.01\n",
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "optimizer = \"adagrad\"\n",
        "beam_width = 1\n",
        "bidirectional_type = True\n",
        "length_penalty = 0.6\n",
        "teacher_forcing = 0.5\n",
        "total_params = 0\n",
        "\n",
        "# Create train data batch\n",
        "val_batch_x = torch.split(val_ip, batch_size, dim=1)\n",
        "val_batch_y =  torch.split(val_op, batch_size, dim=1)\n",
        "train_batch_x = torch.split(train_ip, batch_size, dim=1)\n",
        "train_batch_y =  torch.split(train_op, batch_size, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "# Intialize encoder, decoder and seq2seq model\n",
        "encoder = EncoderModule(input_size, embedding_size, hidden_size, enc_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "decoder = DecoderModule(output_size, embedding_size, hidden_size, output_size, dec_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "model = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, module_type).to(device)\n",
        "\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.requires_grad:\n",
        "        total_params += p.numel()\n",
        "\n",
        "print(model)\n",
        "print(f'Total Trainable Parameters: {total_params}')\n",
        "\n",
        "\n",
        "# Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = assign_opt(optimizer)\n",
        "\n",
        "\n",
        "# TRAINING\n",
        "model, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UgjqMnPF5l0O",
      "metadata": {
        "id": "UgjqMnPF5l0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb708a75-09f9-4260-806f-c1d97815dd4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af84012c",
      "metadata": {
        "id": "af84012c"
      },
      "source": [
        "## SWEEP CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748dc28e",
      "metadata": {
        "id": "748dc28e"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     wandb.init(project='DL_Assignment_3')\n",
        "#     config = wandb.config\n",
        "#     wandb.run.name = 'cell_' + config.module_type + '_bs_' + str(config.batch_size) + '_ep_' + str(config.num_epochs) + '_op_' + str(config.optimizer) + '_drop_' + str(config.dropout) + '_bsw_' + str(config.beam_search_width) +'_emb_' + str(config.embedding_size) + '_hs_' + str(config.hidden_size) + '_elayer_' + str(config.layers) + '_dlayer_' + str(config.layers)\n",
        "\n",
        "#     # Load Dataset\n",
        "#     df_train, train_input_len, train_out_len = load_dataset('/kaggle/input/hinid-dataset/aksharantar_sampled/hin/hin_train.csv')\n",
        "#     df_val, val_input_len, val_out_len = load_dataset('/kaggle/input/hinid-dataset/aksharantar_sampled/hin/hin_valid.csv')\n",
        "#     df_test, test_input_len, test_out_len = load_dataset('/kaggle/input/hinid-dataset/aksharantar_sampled/hin/hin_test.csv')\n",
        "\n",
        "#     input_max_len = max(train_input_len, val_input_len, test_input_len)\n",
        "#     output_max_len = max(train_out_len, val_out_len, test_out_len)\n",
        "\n",
        "#     max_length = max(input_max_len, output_max_len)\n",
        "\n",
        "#     # Create Look Up Table\n",
        "#     input_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\n",
        "#     output_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n",
        "\n",
        "#     # Data Embedding and Converting them into Tensor\n",
        "#     train_inputs, train_outputs = get_tensor_object(df_train, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "#     val_inputs, val_outputs = get_tensor_object(df_val, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "#     test_inputs, test_outputs = get_tensor_object(df_test, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "\n",
        "#     # Transpose column wise\n",
        "#     train_inputs, train_outputs = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\n",
        "#     val_inputs, val_outputs = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\n",
        "#     test_inputs, test_outputs = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n",
        "\n",
        "\n",
        "#     # Initialize Hyperparameters\n",
        "#     input_size = len(input_char_to_int)\n",
        "#     output_size = len(output_char_to_int)\n",
        "#     embedding_size = config.embedding_size\n",
        "#     hidden_size = config.hidden_size\n",
        "#     enc_layers = config.layers\n",
        "#     dec_layers = config.layers\n",
        "#     module_type = config.module_type\n",
        "#     dropout = config.dropout\n",
        "#     learning_rate = config.learning_rate\n",
        "#     batch_size = config.batch_size\n",
        "#     num_epochs = config.num_epochs\n",
        "#     optimizer = config.optimizer\n",
        "#     beam_width = config.beam_search_width\n",
        "#     bidirectional_type = config.bidirectional_type\n",
        "#     length_penalty = config.length_penalty\n",
        "#     teacher_forcing = config.teacher_forcing\n",
        "#     learning_rate = config.learning_rate\n",
        "\n",
        "#     # Create train data batch\n",
        "#     train_batch_x, train_batch_y = torch.split(train_inputs, batch_size, dim=1), torch.split(train_outputs, batch_size, dim=1)\n",
        "#     # Validation data batch\n",
        "#     val_batch_x, val_batch_y = torch.split(val_inputs, batch_size, dim=1), torch.split(val_outputs, batch_size, dim=1)\n",
        "\n",
        "\n",
        "#     # Intialize encoder, decoder and seq2seq model\n",
        "#     encoder = EncoderModule(input_size, embedding_size, hidden_size, enc_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "#     decoder = DecoderModule(output_size, embedding_size, hidden_size, output_size, dec_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "#     model = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, module_type).to(device)\n",
        "\n",
        "#     # Print total number of parameters in the model\n",
        "#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#     print(model)\n",
        "#     print(f'Total Trainable Parameters: {total_params}')\n",
        "\n",
        "\n",
        "#     # Loss function and Optimizer\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     if optimizer == 'adam':\n",
        "#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'sgd':\n",
        "#         optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'rmsprop':\n",
        "#         optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'nadam':\n",
        "#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'adagrad':\n",
        "#         optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "#     else:\n",
        "#         print(\"Incorrect Optmizer !!!!\")\n",
        "\n",
        "#     # TRAINING\n",
        "#     model, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length, 1)\n",
        "#     wandb.log({\n",
        "#             \"accuracy\": acc,\n",
        "#         })\n",
        "\n",
        "# # SWEEP CONFIG\n",
        "# sweep_config = {\n",
        "#     'name': 'sweep_1',\n",
        "#     'method': 'grid',\n",
        "#     'metric': {'name': 'accuracy', 'goal': 'maximize'},\n",
        "#     'parameters': {\n",
        "#         'embedding_size': {'values': [64, 256]},\n",
        "#         'hidden_size': {'values': [256, 512]},\n",
        "#         'layers': {'values': [2, 3]},\n",
        "#         'module_type': {'values':['LSTM', \"GRU\", \"RNN\"]}, # RNN, LSTM, GRU\n",
        "#         'dropout': {'values': [0.3, 0.5]},\n",
        "#         'learning_rate': {'values': [0.01, 0.001]},\n",
        "#         'batch_size': {'values': [32]},\n",
        "#         'num_epochs': {'values': [10]},\n",
        "#         'optimizer': {'values': ['sgd', 'rmsprop', 'adam', 'nadam', 'adagrad']}, # ['sgd', 'rmsprop', 'adam', 'nadam']\n",
        "#         'beam_search_width': {'values': [1, 3, 5]},\n",
        "#         'length_penalty' : {'values': [0.6]},\n",
        "#         'bidirectional_type': {'values': [False, True]},\n",
        "#         'teacher_forcing': {'values': [0.5, 0.7]}\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# # RUN SWEEP ID with agent\n",
        "# sweep_id = wandb.sweep(sweep_config, project = 'DL_Assignment_3')\n",
        "# wandb.agent(sweep_id, main, count = 30)\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41f74d6",
      "metadata": {
        "id": "e41f74d6"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ac9bb0",
      "metadata": {
        "id": "36ac9bb0"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d299620c",
      "metadata": {
        "id": "d299620c"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     parser.add_argument('-dp', '--data_path', type=str, default='kaggle/input/hinid-dataset/aksharantar_sampled/hin', help='Path to the data folder')\n",
        "#     parser.add_argument('-l', '--lang', type=str, default='hin', help='Language for which training is to be done')\n",
        "#     parser.add_argument('-es', '--embedding_size', type=int, default=256, help='Embedding size')\n",
        "#     parser.add_argument('-hs', '--hidden_size', type=int, default=512, help='Hidden size')\n",
        "#     parser.add_argument('-nl', '--layers', type=int, default=2, help='Number of layers')\n",
        "#     parser.add_argument('-ct', '--module_type', type=str, default='LSTM', choices=['RNN', 'LSTM', 'GRU'], help='Cell type (RNN, LSTM, GRU)')\n",
        "#     parser.add_argument('-dr', '--dropout', type=float, default=0.3, help='Dropout rate')\n",
        "#     parser.add_argument('-lr', '--learning_rate', type=float, default=0.01, help='Learning rate')\n",
        "#     parser.add_argument('-bs', '--batch_size', type=int, default=32, help='Batch size')\n",
        "#     parser.add_argument('-ne', '--num_epochs', type=int, default=10, help='Number of epochs')\n",
        "#     parser.add_argument('-op', '--optimizer', type=str, default='adagrad', choices=['adam', 'sgd', 'rmsprop', 'nadam', 'adagrad'], help='Optimizer (adam, sgd, rmsprop, nadam, adagrad)')\n",
        "#     parser.add_argument('-bw', '--beam_search_width', type=int, default=1, help='Beam search width')\n",
        "#     parser.add_argument('-lp', '--length_penalty', type=float, default=0.6, help='Length penalty')\n",
        "#     parser.add_argument('-tf', '--teacher_forcing', type=float, default=0.7, help='Teacher forcing ratio')\n",
        "#     parser.add_argument('-bi', '--bidirectional_type', action='store_true', default=True, help='Use bidirectional_type encoder')\n",
        "#     parser.add_argument('--wandb_log', type=int, default=0, help='Whether to log to WandB (1 for yes, 0 for no)')\n",
        "\n",
        "\n",
        "#     config = parser.parse_args()\n",
        "#     data_path = config.data_path\n",
        "#     lang = config.lang\n",
        "\n",
        "\n",
        "#     # Load Dataset\n",
        "#     df_train, train_input_len, train_out_len = load_dataset(f'/{data_path}/{lang}/{lang}_train.csv')\n",
        "#     df_val, val_input_len, val_out_len = load_dataset(f'/{data_path}/{lang}/{lang}_valid.csv')\n",
        "#     df_test, test_input_len, test_out_len = load_dataset(f'/{data_path}/{lang}/{lang}_test.csv')\n",
        "\n",
        "#     input_max_len = max(train_input_len, val_input_len, test_input_len)\n",
        "#     output_max_len = max(train_out_len, val_out_len, test_out_len)\n",
        "\n",
        "#     max_length = max(input_max_len, output_max_len)\n",
        "\n",
        "#     # Create Look Up Table\n",
        "#     input_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\n",
        "#     output_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n",
        "\n",
        "#     # Data Embedding and Converting them into Tensor\n",
        "#     train_inputs, train_outputs = get_tensor_object(df_train, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "#     val_inputs, val_outputs = get_tensor_object(df_val, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "#     test_inputs, test_outputs = get_tensor_object(df_test, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "\n",
        "#     # Transpose column wise\n",
        "#     train_inputs, train_outputs = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\n",
        "#     val_inputs, val_outputs = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\n",
        "#     test_inputs, test_outputs = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n",
        "\n",
        "#     # Initialize Hyperparameters\n",
        "#     input_size = len(input_char_to_int)\n",
        "#     output_size = len(output_char_to_int)\n",
        "#     embedding_size = config.embedding_size\n",
        "#     hidden_size = config.hidden_size\n",
        "#     enc_layers = config.layers\n",
        "#     dec_layers = config.layers\n",
        "#     module_type = config.module_type\n",
        "#     dropout = config.dropout\n",
        "#     learning_rate = config.learning_rate\n",
        "#     batch_size = config.batch_size\n",
        "#     num_epochs = config.num_epochs\n",
        "#     optimizer = config.optimizer\n",
        "#     beam_width = config.beam_search_width\n",
        "#     bidirectional_type = config.bidirectional_type\n",
        "#     length_penalty = config.length_penalty\n",
        "#     teacher_forcing = config.teacher_forcing\n",
        "#     learning_rate = config.learning_rate\n",
        "\n",
        "#     # Create train data batch\n",
        "#     train_batch_x, train_batch_y = torch.split(train_inputs, batch_size, dim=1), torch.split(train_outputs, batch_size, dim=1)\n",
        "#     # Validation data batch\n",
        "#     val_batch_x, val_batch_y = torch.split(val_inputs, batch_size, dim=1), torch.split(val_outputs, batch_size, dim=1)\n",
        "\n",
        "\n",
        "#     # Intialize encoder, decoder and seq2seq model\n",
        "#     encoder = EncoderModule(input_size, embedding_size, hidden_size, enc_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "#     decoder = DecoderModule(output_size, embedding_size, hidden_size, output_size, dec_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "#     model = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, module_type).to(device)\n",
        "\n",
        "#     # Print total number of parameters in the model\n",
        "#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#     print(model)\n",
        "#     print(f'Total Trainable Parameters: {total_params}')\n",
        "\n",
        "\n",
        "#     # Loss function and Optimizer\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     if optimizer == 'adam':\n",
        "#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'sgd':\n",
        "#         optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'rmsprop':\n",
        "#         optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'nadam':\n",
        "#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'adagrad':\n",
        "#         optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#     # TRAINING\n",
        "\n",
        "#     if config.wandb_log == 1:\n",
        "#         wandb.init(project='DL_Assignment_3')\n",
        "#         wandb.run.name = 'cell_' + config.module_type + '_bs_' + str(config.batch_size) + '_ep_' + str(config.num_epochs) + '_op_' + str(config.optimizer) + '_drop_' + str(config.dropout) + '_bsw_' + str(config.beam_search_width) +'_emb_' + str(config.embedding_size) + '_hs_' + str(config.hidden_size) + '_elayer_' + str(config.layers) + '_dlayer_' + str(config.layers)\n",
        "\n",
        "#     model, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length, config.wandb_log)\n",
        "#     if config.wandb_log == 1:\n",
        "#         wandb.log({\n",
        "#                 \"accuracy\": acc,\n",
        "#             })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3318a63",
      "metadata": {
        "id": "c3318a63"
      },
      "outputs": [],
      "source": [
        "# # Example usage\n",
        "# for i in range(10):\n",
        "#     input_seq = df_train.iloc[i, 0][:-1]\n",
        "#     predicted_output = beam_search(model, input_seq, input_char_to_int, output_char_to_int, output_int_to_char, 1, 0.6, \"RNN\")\n",
        "\n",
        "#     print(f\"Input Sequence {i+1}: {input_seq}\")\n",
        "#     print(f\"Predicted Output Sequence {i+1}: {predicted_output}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18596e2e",
      "metadata": {
        "id": "18596e2e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb19ba0d",
      "metadata": {
        "id": "bb19ba0d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "126c0e23",
      "metadata": {
        "id": "126c0e23"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 883208,
          "sourceId": 1508471,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4915921,
          "sourceId": 8278287,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 27738.475675,
      "end_time": "2024-05-04T03:05:13.889617",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-05-03T19:22:55.413942",
      "version": "2.5.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}