{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac9d613",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-05-03T19:22:58.202406Z",
          "iopub.status.busy": "2024-05-03T19:22:58.202094Z",
          "iopub.status.idle": "2024-05-03T19:23:03.591914Z",
          "shell.execute_reply": "2024-05-03T19:23:03.590997Z"
        },
        "papermill": {
          "duration": 5.399777,
          "end_time": "2024-05-03T19:23:03.594224",
          "exception": false,
          "start_time": "2024-05-03T19:22:58.194447",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ac9d613",
        "outputId": "d217cdb7-a31a-4c2c-c277-2b7e5969215e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import Lib\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import random\n",
        "import heapq\n",
        "\n",
        "# Set device (CUDA if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24b70a64",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-03T19:23:03.607670Z",
          "iopub.status.busy": "2024-05-03T19:23:03.607380Z",
          "iopub.status.idle": "2024-05-03T19:23:06.474926Z",
          "shell.execute_reply": "2024-05-03T19:23:06.473793Z"
        },
        "papermill": {
          "duration": 2.876934,
          "end_time": "2024-05-03T19:23:06.477410",
          "exception": false,
          "start_time": "2024-05-03T19:23:03.600476",
          "status": "completed"
        },
        "tags": [],
        "id": "24b70a64"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8f99e297",
      "metadata": {
        "papermill": {
          "duration": 0.006732,
          "end_time": "2024-05-03T19:23:06.491581",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.484849",
          "status": "completed"
        },
        "tags": [],
        "id": "8f99e297"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e6838a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-03T19:23:06.507567Z",
          "iopub.status.busy": "2024-05-03T19:23:06.507207Z",
          "iopub.status.idle": "2024-05-03T19:23:06.669344Z",
          "shell.execute_reply": "2024-05-03T19:23:06.668544Z"
        },
        "papermill": {
          "duration": 0.172917,
          "end_time": "2024-05-03T19:23:06.671376",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.498459",
          "status": "completed"
        },
        "tags": [],
        "id": "22e6838a"
      },
      "outputs": [],
      "source": [
        "class lang:\n",
        "    def __init__(self,path_train,path_val,path_test):\n",
        "        self.path_train = path_train\n",
        "        self.path_val = path_val\n",
        "        self.path_test = path_test\n",
        "        self.trainfile = pd.read_csv(path_train,header=None, encoding='utf-8')\n",
        "        self.valfile = pd.read_csv(path_val,header=None, encoding='utf-8')\n",
        "        self.testfile = pd.read_csv(path_test,header=None, encoding='utf-8')\n",
        "\n",
        "    def datasetencoder(self,file):\n",
        "        file[0] = file[0].apply(lambda x: x + '>')\n",
        "        file[1] = file[1].apply(lambda x: '<' + x + '>')\n",
        "\n",
        "        # Calculate maximum length of unique elements in file[0]\n",
        "        ipmax = 0\n",
        "        for x in file[0].unique():\n",
        "            if len(x) > ipmax:\n",
        "                ipmax = len(x)\n",
        "\n",
        "        # Calculate maximum length of unique elements in file[1]\n",
        "        opmax = 0\n",
        "        for x in file[1].unique():\n",
        "            if len(x) > opmax:\n",
        "                opmax = len(x)\n",
        "\n",
        "\n",
        "        return ipmax,opmax,file\n",
        "\n",
        "    def dictionary_create(self,data):\n",
        "        data.discard('<')\n",
        "        data.discard('>')\n",
        "        chartoint = {\"\": 0, '<':1, '>':2}\n",
        "        inttochar = {}\n",
        "\n",
        "        for ci, c in enumerate(sorted(data), len(chartoint)):\n",
        "            chartoint[c] = ci\n",
        "        for c, ci in chartoint.items():\n",
        "            inttochar[ci] = c\n",
        "\n",
        "        return chartoint,inttochar\n",
        "\n",
        "    def convert_tensor_element(self,data , length , chartoint):\n",
        "\n",
        "        data_enc = np.zeros(length)\n",
        "        encoder = []\n",
        "        for char in data:\n",
        "            encoder.append(chartoint[char])\n",
        "        encoder = np.array(encoder)\n",
        "        length = min(length, len(encoder))\n",
        "        data_enc[:length] = encoder[:length]\n",
        "\n",
        "        return torch.tensor(data_enc, dtype=torch.int64)\n",
        "\n",
        "    def convert_tensor_data(self,data,maxlength_ip, chartoint_ip,maxlength_op, chartoint_op):\n",
        "\n",
        "        tensor_obj_input = []\n",
        "        tensor_obj_output = []\n",
        "\n",
        "        for ip, op in zip(data[0], data[1]):\n",
        "            # Encode input string\n",
        "            temp_input = self.convert_tensor_element(ip, maxlength_ip, chartoint_ip)\n",
        "            tensor_obj_input.append(temp_input)\n",
        "\n",
        "            # Encode output string\n",
        "            temp_output = self.convert_tensor_element(op, maxlength_op, chartoint_op)\n",
        "            tensor_obj_output.append(temp_output)\n",
        "\n",
        "        tensor_obj_input =  torch.stack(tensor_obj_input)\n",
        "        tensor_obj_output = torch.stack(tensor_obj_output)\n",
        "\n",
        "\n",
        "        return tensor_obj_input , tensor_obj_output\n",
        "\n",
        "    def preparedata(self):\n",
        "\n",
        "        train_ipmax , train_opmax , train = self.datasetencoder(self.trainfile)\n",
        "        val_ipmax , val_opmax , val =self.datasetencoder(self.valfile)\n",
        "        test_ipmax , test_opmax , test =self.datasetencoder(self.testfile)\n",
        "\n",
        "        input_char_to_int,input_int_to_char  = self.dictionary_create(set(''.join(train[0]) + ''.join(val[0]) + ''.join(test[0])))\n",
        "        output_char_to_int ,output_int_to_char= self.dictionary_create(set(''.join(train[1]) + ''.join(val[1]) + ''.join(test[1])))\n",
        "\n",
        "        # print(input_char_to_int)\n",
        "        # print(output_char_to_int)\n",
        "\n",
        "        ipmax = max(train_ipmax ,val_ipmax ,test_ipmax)\n",
        "        opmax = max(train_opmax ,val_opmax , test_opmax)\n",
        "\n",
        "        train_tensor_ip, train_tensor_op = self.convert_tensor_data(train,ipmax,input_char_to_int,opmax,output_char_to_int)\n",
        "        val_tensor_ip, val_tensor_op = self.convert_tensor_data(val,ipmax,input_char_to_int,opmax,output_char_to_int)\n",
        "        test_tensor_ip, test_tensor_op = self.convert_tensor_data(test,ipmax,input_char_to_int,opmax,output_char_to_int)\n",
        "\n",
        "        #transpose data tensor\n",
        "\n",
        "        train_tensor_ip, train_tensor_op = train_tensor_ip.t(),train_tensor_op.t()\n",
        "        val_tensor_ip, val_tensor_op = val_tensor_ip.t(), val_tensor_op.t()\n",
        "        test_tensor_ip, test_tensor_op = test_tensor_ip.t() , test_tensor_op.t()\n",
        "\n",
        "        len_max = max(ipmax,opmax)\n",
        "\n",
        "        return train_tensor_ip, train_tensor_op ,val_tensor_ip, val_tensor_op,test_tensor_ip, test_tensor_op,input_char_to_int,input_int_to_char,output_char_to_int ,output_int_to_char,val,len_max\n",
        "\n",
        "        #do all func call in this and return final datasets\n",
        "\n",
        "\n",
        "# pathtrain = \"/kaggle/input/akshatantra/aksharantar_sampled/hin/hin_train.csv\"\n",
        "# pathval = \"/kaggle/input/akshatantra/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "# pathtest = \"/kaggle/input/akshatantra/aksharantar_sampled/hin/hin_test.csv\"\n",
        "\n",
        "# language = lang(pathtrain , pathval, pathtest)\n",
        "# train_ip, train_op ,val_ip, val_op,test_ip, test_op,input_char_to_int,input_int_to_char,output_char_to_int ,output_int_to_char,df_val,len_max = language.preparedata()\n",
        "\n",
        "# print(val_ip.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34f82f7e",
      "metadata": {
        "papermill": {
          "duration": 0.005863,
          "end_time": "2024-05-03T19:23:06.683530",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.677667",
          "status": "completed"
        },
        "tags": [],
        "id": "34f82f7e"
      },
      "source": [
        "# Create Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a77bee6",
      "metadata": {
        "papermill": {
          "duration": 0.006578,
          "end_time": "2024-05-03T19:23:06.696130",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.689552",
          "status": "completed"
        },
        "tags": [],
        "id": "7a77bee6"
      },
      "source": [
        "## encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e7ffcdb8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-03T19:23:06.709928Z",
          "iopub.status.busy": "2024-05-03T19:23:06.709085Z",
          "iopub.status.idle": "2024-05-03T19:23:06.730320Z",
          "shell.execute_reply": "2024-05-03T19:23:06.729651Z"
        },
        "papermill": {
          "duration": 0.030115,
          "end_time": "2024-05-03T19:23:06.732223",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.702108",
          "status": "completed"
        },
        "tags": [],
        "id": "e7ffcdb8"
      },
      "outputs": [],
      "source": [
        "class EncoderModule(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, layers, dropout, bidirectional, module_type):\n",
        "        super(EncoderModule, self).__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layers = layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.module_type = module_type\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "\n",
        "        if module_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        if module_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        if module_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)\n",
        "\n",
        "\n",
        "    def forward(self, x): # x shape: (seq_length, N) where N is batch size\n",
        "        # Perform dropout on the input\n",
        "        embedding = self.embedding(x)\n",
        "        embedding = self.dropout(embedding) # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "        if self.module_type == \"LSTM\":\n",
        "            # Pass through the LSTM layer\n",
        "            outputs, (hidden, cell) = self.rnn(embedding) # outputs shape: (seq_length, N, hidden_size)\n",
        "            if self.bidirectional == True:\n",
        "                # Sum the bidirectional outputs\n",
        "                outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "                hidden = torch.cat((hidden[: self.layers], hidden[self.layers:]), dim=0)\n",
        "            # Return hidden state and cell state\n",
        "            return hidden, cell\n",
        "\n",
        "        if self.module_type == \"GRU\" :\n",
        "            # Pass through the RNN/GRU layer\n",
        "            outputs, hidden = self.rnn(embedding) # outputs shape: (seq_length, N, hidden_size)\n",
        "            if self.bidirectional == True:\n",
        "                # Sum the bidirectional outputs\n",
        "                outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "                hidden = torch.cat((hidden[: self.layers], hidden[self.layers:]), dim=0)\n",
        "\n",
        "            # Return hidden state and cell state\n",
        "            return hidden\n",
        "\n",
        "        if self.module_type == \"RNN\":\n",
        "            # Pass through the RNN/GRU layer\n",
        "            outputs, hidden = self.rnn(embedding) # outputs shape: (seq_length, N, hidden_size)\n",
        "            if self.bidirectional == True:\n",
        "                # Sum the bidirectional outputs\n",
        "                outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "                hidden = torch.cat((hidden[: self.layers], hidden[self.layers:]), dim=0)\n",
        "\n",
        "            # Return hidden state and cell state\n",
        "            return hidden\n",
        "\n",
        "\n",
        "class DecoderModule(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, layers, dropout, bidirectional, module_type):\n",
        "        super(DecoderModule, self).__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layers = layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.module_type = module_type\n",
        "\n",
        "        # Define embedding layer\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "        # Define RNN layer with specific cell type\n",
        "        if module_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        if module_type == 'GRU':\n",
        "            self.rnn = nn.GRU(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        if module_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)\n",
        "\n",
        "\n",
        "        if bidirectional:\n",
        "          input_size = hidden_size * 2\n",
        "        else:\n",
        "          input_size = hidden_size\n",
        "\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden, cell): # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "\n",
        "        # Ensure x has the shape (1, N)\n",
        "\n",
        "        x = x.unsqueeze(0)\n",
        "        # Perform dropout on the input\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))  # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        if self.module_type == \"LSTM\":\n",
        "            # Pass through the LSTM layer\n",
        "            outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))  # outputs shape: (1, N, hidden_size * num_directions)\n",
        "\n",
        "            # Pass through fully connected layer\n",
        "            out = self.fc(outputs).squeeze(0)\n",
        "            predictions = self.log_softmax(out)\n",
        "\n",
        "            return predictions, hidden, cell\n",
        "        if self.module_type == \"GRU\":\n",
        "            # Pass through the RNN/GRU layer\n",
        "            outputs, hidden = self.rnn(embedding, hidden)  # outputs shape: (1, N, hidden_size * num_directions)\n",
        "\n",
        "            # Pass through fully connected layer\n",
        "            out = self.fc(outputs).squeeze(0)\n",
        "            predictions = self.log_softmax(out)\n",
        "\n",
        "            return predictions, hidden\n",
        "\n",
        "        if self.module_type == \"RNN\":\n",
        "            # Pass through the RNN/GRU layer\n",
        "            outputs, hidden = self.rnn(embedding, hidden)  # outputs shape: (1, N, hidden_size * num_directions)\n",
        "\n",
        "            # Pass through fully connected layer\n",
        "            out = self.fc(outputs).squeeze(0)\n",
        "            predictions = self.log_softmax(out)\n",
        "\n",
        "            return predictions, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97aa96f9",
      "metadata": {
        "papermill": {
          "duration": 0.00565,
          "end_time": "2024-05-03T19:23:06.743839",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.738189",
          "status": "completed"
        },
        "tags": [],
        "id": "97aa96f9"
      },
      "source": [
        "## Seq2Seq Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "05a2ab9f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-03T19:23:06.757104Z",
          "iopub.status.busy": "2024-05-03T19:23:06.756352Z",
          "iopub.status.idle": "2024-05-03T19:23:06.765833Z",
          "shell.execute_reply": "2024-05-03T19:23:06.764999Z"
        },
        "papermill": {
          "duration": 0.017972,
          "end_time": "2024-05-03T19:23:06.767676",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.749704",
          "status": "completed"
        },
        "tags": [],
        "id": "05a2ab9f"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, output_char_to_int, teacher_forcing, module_type):\n",
        "\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        # Initialize encoder and decoder\n",
        "        self.decoder = decoder\n",
        "        self.encoder = encoder\n",
        "        self.module_type = module_type\n",
        "        self.target_vocab_size = len(output_char_to_int)\n",
        "        self.teacher_force_ratio = teacher_forcing\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        # Get batch size, target length, and target vocabulary size\n",
        "        batch_size = source.shape[1]\n",
        "\n",
        "        target_vocab_size = self.target_vocab_size\n",
        "        teacher_force_ratio = self.teacher_force_ratio\n",
        "\n",
        "        # Initialize outputs tensor\n",
        "        outputs = torch.zeros(target.shape[0], batch_size, target_vocab_size).to(source.device)\n",
        "\n",
        "        x = target[0]\n",
        "        # Get hidden state and cell state from encoder\n",
        "        if self.module_type == 'LSTM':\n",
        "            hidden, cell = self.encoder(source)\n",
        "        if self.module_type == 'GRU':\n",
        "            hidden = self.encoder(source)\n",
        "        if self.module_type == 'RNN':\n",
        "            hidden = self.encoder(source)\n",
        "\n",
        "\n",
        "        for t in range(1, target.shape[0]):\n",
        "            # Use previous hidden and cell states as context from encoder at start\n",
        "            if self.module_type == 'LSTM':\n",
        "                output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            if self.module_type == 'GRU':\n",
        "                output, hidden = self.decoder(x, hidden, None)\n",
        "            if self.module_type == 'RNN':\n",
        "                output, hidden = self.decoder(x, hidden, None)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t] = output\n",
        "            # Get the best word the DecoderModule predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "            # Update input for next time step based on teacher forcing ratio\n",
        "            if random.random() >= teacher_force_ratio:\n",
        "              x = best_guess\n",
        "            else:\n",
        "              x = target[t]\n",
        "\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958e4d20",
      "metadata": {
        "papermill": {
          "duration": 0.005923,
          "end_time": "2024-05-03T19:23:06.779445",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.773522",
          "status": "completed"
        },
        "tags": [],
        "id": "958e4d20"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "dd523c22",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-03T19:23:06.792691Z",
          "iopub.status.busy": "2024-05-03T19:23:06.792408Z",
          "iopub.status.idle": "2024-05-03T19:23:06.823560Z",
          "shell.execute_reply": "2024-05-03T19:23:06.822748Z"
        },
        "papermill": {
          "duration": 0.040066,
          "end_time": "2024-05-03T19:23:06.825430",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.785364",
          "status": "completed"
        },
        "tags": [],
        "id": "dd523c22"
      },
      "outputs": [],
      "source": [
        "def beam_search(model, input_seq, max_length, input_char_index, output_char_index, reverse_target_char_index, beam_width, length_penalty, cell_type):\n",
        "    \"\"\"\n",
        "    Perform beam search to generate a sequence using the provided model.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The Seq2Seq model.\n",
        "    - input_seq (str): The input sequence.\n",
        "    - max_length (int): Maximum length of the input sequence.\n",
        "    - input_char_index (dict): Mapping from characters to integers for the input vocabulary.\n",
        "    - output_char_index (dict): Mapping from characters to integers for the output vocabulary.\n",
        "    - reverse_target_char_index (dict): Reverse mapping from integers to characters for the output vocabulary.\n",
        "    - beam_width (int): Beam width for beam search.\n",
        "    - length_penalty (float): Length penalty for beam search.\n",
        "    - cell_type (str): Type of RNN cell used in the model ('LSTM', 'GRU', or 'RNN').\n",
        "\n",
        "    Returns:\n",
        "    - str: The generated output sequence.\n",
        "    \"\"\"\n",
        "    if len(input_seq) > max_length:\n",
        "        print(\"Input Length is exceeding max length!!!!\")\n",
        "        return \"\"\n",
        "\n",
        "    # Create np array of zeros of length input\n",
        "    input_data = np.zeros((max_length, 1), dtype=int)  # (N,1)\n",
        "\n",
        "    # Encode the input\n",
        "    for idx, char in enumerate(input_seq):\n",
        "        input_data[idx, 0] = input_char_index[char]\n",
        "    input_data[idx + 1, 0] = input_char_index[\">\"]  # EOS\n",
        "\n",
        "    # Convert to tensor\n",
        "    input_tensor = torch.tensor(input_data, dtype=torch.int64).to(device)  # N,1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if cell_type == 'LSTM':\n",
        "            hidden, cell = model.encoder(input_tensor)\n",
        "\n",
        "        else:\n",
        "            hidden = model.encoder(input_tensor)\n",
        "\n",
        "    # Initialize beam\n",
        "    out_t = output_char_index['<']\n",
        "    out_reshape = np.array(out_t).reshape(1,)\n",
        "    hidden_par = hidden.unsqueeze(0)\n",
        "    initial_sequence = torch.tensor(out_reshape).to(device)\n",
        "    beam = [(0.0, initial_sequence, hidden_par)]  # [(score, sequence, hidden)]\n",
        "\n",
        "    for _ in range(len(output_char_index)):\n",
        "        candidates = []\n",
        "        for score, seq, hidden in beam:\n",
        "            if seq[-1].item() == output_char_index['>']:\n",
        "                # If the sequence ends with the end token, add it to the candidates\n",
        "                candidates.append((score, seq, hidden))\n",
        "                continue\n",
        "\n",
        "            last_token = np.array(seq[-1].item()).reshape(1,)\n",
        "            x = torch.tensor(last_token).to(device)\n",
        "\n",
        "            if cell_type == 'LSTM':\n",
        "                output, hidden, cell,  = model.decoder(x, hidden.squeeze(0), cell)\n",
        "            else:\n",
        "                output, hidden,  = model.decoder(x, hidden.squeeze(0), None)\n",
        "\n",
        "            probabilities = F.softmax(output, dim=1)\n",
        "            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n",
        "\n",
        "            for prob, token in zip(topk_probs[0], topk_tokens[0]):\n",
        "                new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n",
        "                seq_length_norm_factor = (len(new_seq) - 1) / 5\n",
        "                candidate_score = score + torch.log(prob).item() / (seq_length_norm_factor ** length_penalty)\n",
        "                candidates.append((candidate_score, new_seq, hidden.unsqueeze(0)))\n",
        "\n",
        "        # Select top-k candidates based on the accumulated scores\n",
        "        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n",
        "\n",
        "    best_score, best_sequence, _ = max(beam, key=lambda x: x[0])  # Select the best sequence from the beam as the output\n",
        "\n",
        "    # Convert the best sequence indices to characters\n",
        "    return ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:]])\n",
        "\n",
        "\n",
        "\n",
        "# TRAINING FUNCTION\n",
        "def train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length, wandb_log):\n",
        "    \"\"\"\n",
        "    Train the Seq2Seq model.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The Seq2Seq model.\n",
        "    - num_epochs (int): Number of training epochs.\n",
        "    - criterion: Loss criterion for training.\n",
        "    - optimizer: Optimizer for training.\n",
        "    - train_batch_x: Training input data.\n",
        "    - train_batch_y: Training target data.\n",
        "    - val_batch_x: Validation input data.\n",
        "    - val_batch_y: Validation target data.\n",
        "    - df_val: DataFrame for validation data.\n",
        "    - input_char_to_int (dict): Mapping from characters to integers for the input vocabulary.\n",
        "    - output_char_to_int (dict): Mapping from characters to integers for the output vocabulary.\n",
        "    - output_int_to_char (dict): Reverse mapping from integers to characters for the output vocabulary.\n",
        "    - beam_width (int): Beam width for beam search.\n",
        "    - length_penalty (float): Length penalty for beam search.\n",
        "    - module_type (str): Type of RNN cell used in the model ('LSTM', 'GRU', or 'RNN').\n",
        "    - max_length (int): Maximum length of sequences.\n",
        "    - wandb_log (int): Whether to log to wandb (1 or 0).\n",
        "    Returns:\n",
        "    - nn.Module: The trained model.\n",
        "    - float: Validation accuracy.\n",
        "    \"\"\"\n",
        "    for epoch in range(num_epochs):\n",
        "        total_words = 0\n",
        "        correct_pred = 0\n",
        "        total_loss = 0\n",
        "        accuracy = 0\n",
        "        model.train()\n",
        "\n",
        "        # Use tqdm for progress tracking\n",
        "        train_data_iterator = tqdm(zip(train_batch_x, train_batch_y), total=len(train_batch_x))\n",
        "\n",
        "        for (x, y) in train_data_iterator:\n",
        "            # Get input and targets and move to device\n",
        "            target, inp_data = y.to(device), x.to(device)\n",
        "\n",
        "            # Forward propagation\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inp_data, target)\n",
        "\n",
        "            target = target.reshape(-1)\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "\n",
        "            pad_mask = (target != 0)\n",
        "            target = target[pad_mask] # Select non-padding elements\n",
        "            output = output[pad_mask]\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate total loss\n",
        "            total_loss = total_loss + loss.item()\n",
        "            # Update total words processed\n",
        "            total_words = total_words + target.size(0)\n",
        "            # Calculate number of correct predictions\n",
        "            correct_pred = correct_pred + torch.sum(torch.argmax(output, dim=1) == target).item()\n",
        "\n",
        "        # Calculate average loss per batch\n",
        "        avg_loss = total_loss / len(train_batch_x)\n",
        "        # Calculate accuracy\n",
        "        acc = correct_pred / total_words\n",
        "        acc *= 100\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_total_loss = 0\n",
        "            val_total_words = 0\n",
        "            val_correct_pred = 0\n",
        "\n",
        "            val_data_iterator = tqdm(zip(val_batch_x, val_batch_y), total=len(val_batch_x))\n",
        "            for x_val, y_val in val_data_iterator:\n",
        "                target_val, inp_data_val = y_val.to(device), x_val.to(device)\n",
        "                output_val = model(inp_data_val, target_val)\n",
        "\n",
        "\n",
        "                target_val = target_val.reshape(-1)\n",
        "                output_val = output_val.reshape(-1, output_val.shape[2])\n",
        "\n",
        "                pad_mask = (target_val != 0)\n",
        "                target_val = target_val[pad_mask] # Select non-padding elements\n",
        "                output_val = output_val[pad_mask]\n",
        "\n",
        "                val_loss = criterion(output_val, target_val)\n",
        "                val_total_loss = val_total_loss+ val_loss.item()\n",
        "                val_total_words = val_total_words+ target_val.size(0)\n",
        "                val_correct_pred = val_correct_pred+ torch.sum(torch.argmax(output_val, dim=1) == target_val).item()\n",
        "\n",
        "            # Calculate validation statistics\n",
        "            val_acc = val_correct_pred / val_total_words\n",
        "            val_acc = 100*val_acc\n",
        "            val_avg_loss = val_total_loss / len(val_batch_x)\n",
        "\n",
        "\n",
        "\n",
        "        # Total word predict correct over training\n",
        "        beam_val_pred = 0\n",
        "        beam_val = 0\n",
        "        for i in tqdm(range(df_val.shape[0])):\n",
        "            input_seq = df_val.iloc[i, 0][:-1]\n",
        "            true_seq = df_val.iloc[i, 1][1:-1]\n",
        "            predicted_output = beam_search(model, input_seq, max_length, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type)\n",
        "            if true_seq == predicted_output[:-1]:\n",
        "                beam_val_pred+=1\n",
        "        beam_val = beam_val_pred/df_val.shape[0]\n",
        "        beam_val = 100*beam_val\n",
        "        # Print statistics\n",
        "\n",
        "        # print(f\"Train Accuracy Char: {accuracy:.4f}, Train Average Loss: {avg_loss:.4f}\")\n",
        "        # print(f\"Validation Accuracy Char: {val_acc:.4f}, Validation Average Loss: {val_avg_loss:.4f}\")\n",
        "        # print(f\"Beam Val Word Accuracy: {beam_val:.4f} Correct Prediction : {beam_val_pred}/{df_val.shape[0]}\")\n",
        "\n",
        "        print(\"========================================================================\")\n",
        "        print(f\"---------------------------- Epoch : \",epoch+1,\"------------------------\")\n",
        "        print(f\"Train accuracy Character: \",acc)\n",
        "        print(f\"Train Average Loss: \",avg_loss)\n",
        "        print(f\"Validation accuracy Character: \",val_acc)\n",
        "        print(f\"Validation Average Loss: \",val_avg_loss)\n",
        "        print(f\"Beam Val Word accuracy: \" ,beam_val)\n",
        "        print(f\"Correct Prediction : {beam_val_pred}/{df_val.shape[0]}\")\n",
        "        print(\"========================================================================\")\n",
        "\n",
        "        # if wandb_log == 1:\n",
        "        #     wandb.log({\n",
        "        #         \"train_accuracy_char\": accuracy,\n",
        "        #         \"train_loss\": avg_loss,\n",
        "        #         \"val_acc_char\": val_acc,\n",
        "        #         \"val_loss\": val_avg_loss,\n",
        "        #         \"beam_val_acc_word\" : beam_val,\n",
        "        #     })\n",
        "\n",
        "\n",
        "    return model, beam_val"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pathtrain = \"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/hin/hin_train.csv\"\n",
        "pathval = \"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/hin/hin_valid.csv\"\n",
        "pathtest = \"/content/drive/MyDrive/aksharantar_sampled/aksharantar_sampled/hin/hin_test.csv\"\n",
        "language = lang(pathtrain , pathval, pathtest)\n",
        "train_ip, train_op ,val_ip, val_op,test_ip, test_op,input_char_to_int,input_int_to_char,output_char_to_int ,output_int_to_char,df_val,max_length = language.preparedata()\n",
        "\n",
        "# df_train, train_input_len, train_out_len = load_dataset(pathtrain)\n",
        "# df_val, val_input_len, val_out_len = load_dataset(pathval)\n",
        "# df_test, test_input_len, test_out_len = load_dataset(pathtest)\n",
        "\n",
        "# input_max_len = max(train_input_len, val_input_len, test_input_len)\n",
        "# output_max_len = max(train_out_len, val_out_len, test_out_len)\n",
        "\n",
        "\n",
        "# # Create Look Up Table\n",
        "# input_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\n",
        "# output_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n",
        "\n",
        "# # print(\"Input Lookup Table:\", input_char_to_int)\n",
        "# # print(\"\\n\\n Output Lookup Table\", output_char_to_int)\n",
        "\n",
        "# # Data Embedding and Converting them into Tensor\n",
        "# train_inputs, train_outputs = get_tensor_object(df_train, input_max_len, input_max_len, input_char_to_int, output_char_to_int)\n",
        "# val_inputs, val_outputs = get_tensor_object(df_val, input_max_len, input_max_len, input_char_to_int, output_char_to_int)\n",
        "# test_inputs, test_outputs = get_tensor_object(df_test, input_max_len, input_max_len, input_char_to_int, output_char_to_int)\n",
        "\n",
        "# # Transpose column wise\n",
        "# train_ip, train_op = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\n",
        "# val_ip, val_op = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\n",
        "# test_ip, test_op = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n",
        "\n",
        "# max_length = max(input_max_len, output_max_len)\n",
        "\n",
        "# Initialize Hyperparameters\n",
        "input_size = len(input_char_to_int)\n",
        "output_size = len(output_char_to_int)\n",
        "embedding_size = 64\n",
        "hidden_size = 256\n",
        "enc_layers = 2\n",
        "dec_layers = 2\n",
        "module_type = \"LSTM\"\n",
        "dropout = 0.3\n",
        "learning_rate = 0.1\n",
        "batch_size = 64\n",
        "num_epochs = 1\n",
        "optimizer = \"adagrad\"\n",
        "beam_width = 1\n",
        "bidirectional_type = True\n",
        "length_penalty = 0.6\n",
        "teacher_forcing = 0.5\n",
        "\n",
        "# Create train data batch\n",
        "train_batch_x, train_batch_y = torch.split(train_ip, batch_size, dim=1), torch.split(train_op, batch_size, dim=1)\n",
        "# Validation data batch\n",
        "val_batch_x, val_batch_y = torch.split(val_ip, batch_size, dim=1), torch.split(val_op, batch_size, dim=1)\n",
        "\n",
        "\n",
        "# Intialize encoder, decoder and seq2seq model\n",
        "encoder = EncoderModule(input_size, embedding_size, hidden_size, enc_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "decoder = DecoderModule(output_size, embedding_size, hidden_size, output_size, dec_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "model = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, module_type).to(device)\n",
        "\n",
        "# Print total number of parameters in the model\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(model)\n",
        "print(f'Total Trainable Parameters: {total_params}')\n",
        "\n",
        "\n",
        "# Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if optimizer == 'adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "elif optimizer == 'sgd':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "elif optimizer == 'rmsprop':\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "elif optimizer == 'nadam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "elif optimizer == 'adagrad':\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# TRAINING\n",
        "model, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length, 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiJhWZu95BtM",
        "outputId": "2761a307-e4d4-4407-92d5-564d4c92fc48"
      },
      "id": "YiJhWZu95BtM",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': 0, '<': 1, '>': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n",
            "{'': 0, '<': 1, '>': 2, 'ँ': 3, 'ं': 4, 'ः': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ए': 13, 'ऐ': 14, 'ऑ': 15, 'ओ': 16, 'औ': 17, 'क': 18, 'ख': 19, 'ग': 20, 'घ': 21, 'ङ': 22, 'च': 23, 'छ': 24, 'ज': 25, 'झ': 26, 'ञ': 27, 'ट': 28, 'ठ': 29, 'ड': 30, 'ढ': 31, 'ण': 32, 'त': 33, 'थ': 34, 'द': 35, 'ध': 36, 'न': 37, 'प': 38, 'फ': 39, 'ब': 40, 'भ': 41, 'म': 42, 'य': 43, 'र': 44, 'ल': 45, 'ळ': 46, 'व': 47, 'श': 48, 'ष': 49, 'स': 50, 'ह': 51, '़': 52, 'ऽ': 53, 'ा': 54, 'ि': 55, 'ी': 56, 'ु': 57, 'ू': 58, 'ृ': 59, 'ॅ': 60, 'े': 61, 'ै': 62, 'ॉ': 63, 'ॊ': 64, 'ो': 65, 'ौ': 66, '्': 67}\n",
            "Seq2Seq(\n",
            "  (decoder): DecoderModule(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (embedding): Embedding(68, 64)\n",
            "    (rnn): LSTM(64, 256, num_layers=2, dropout=0.3, bidirectional=True)\n",
            "    (fc): Linear(in_features=512, out_features=68, bias=True)\n",
            "    (log_softmax): LogSoftmax(dim=1)\n",
            "  )\n",
            "  (encoder): EncoderModule(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (embedding): Embedding(29, 64)\n",
            "    (rnn): LSTM(64, 256, num_layers=2, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            ")\n",
            "Total Trainable Parameters: 4513924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:37<00:00, 21.08it/s]\n",
            "100%|██████████| 64/64 [00:00<00:00, 73.16it/s]\n",
            "100%|██████████| 4096/4096 [00:39<00:00, 103.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "---------------------------- Epoch :  1 ------------------------\n",
            "Train accuracy Character:  37.76266441760726\n",
            "Train Average Loss:  2.370012532174587\n",
            "Validation accuracy Character:  55.421308197657815\n",
            "Validation Average Loss:  1.5726487282663584\n",
            "Beam Val Word accuracy:  12.109375\n",
            "Correct Prediction : 496/4096\n",
            "========================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UgjqMnPF5l0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab08c23-19f8-4d0d-9553-41ab6593b32c"
      },
      "id": "UgjqMnPF5l0O",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af84012c",
      "metadata": {
        "papermill": {
          "duration": 0.005711,
          "end_time": "2024-05-03T19:23:06.837003",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.831292",
          "status": "completed"
        },
        "tags": [],
        "id": "af84012c"
      },
      "source": [
        "## SWEEP CONFIGURATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748dc28e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-03T19:23:06.850154Z",
          "iopub.status.busy": "2024-05-03T19:23:06.849893Z",
          "iopub.status.idle": "2024-05-04T03:00:18.436767Z",
          "shell.execute_reply": "2024-05-04T03:00:18.435763Z"
        },
        "papermill": {
          "duration": 27431.596338,
          "end_time": "2024-05-04T03:00:18.439273",
          "exception": false,
          "start_time": "2024-05-03T19:23:06.842935",
          "status": "completed"
        },
        "tags": [],
        "id": "748dc28e"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     wandb.init(project='DL_Assignment_3')\n",
        "#     config = wandb.config\n",
        "#     wandb.run.name = 'cell_' + config.module_type + '_bs_' + str(config.batch_size) + '_ep_' + str(config.num_epochs) + '_op_' + str(config.optimizer) + '_drop_' + str(config.dropout) + '_bsw_' + str(config.beam_search_width) +'_emb_' + str(config.embedding_size) + '_hs_' + str(config.hidden_size) + '_elayer_' + str(config.layers) + '_dlayer_' + str(config.layers)\n",
        "\n",
        "#     # Load Dataset\n",
        "#     df_train, train_input_len, train_out_len = load_dataset('/kaggle/input/hinid-dataset/aksharantar_sampled/hin/hin_train.csv')\n",
        "#     df_val, val_input_len, val_out_len = load_dataset('/kaggle/input/hinid-dataset/aksharantar_sampled/hin/hin_valid.csv')\n",
        "#     df_test, test_input_len, test_out_len = load_dataset('/kaggle/input/hinid-dataset/aksharantar_sampled/hin/hin_test.csv')\n",
        "\n",
        "#     input_max_len = max(train_input_len, val_input_len, test_input_len)\n",
        "#     output_max_len = max(train_out_len, val_out_len, test_out_len)\n",
        "\n",
        "#     max_length = max(input_max_len, output_max_len)\n",
        "\n",
        "#     # Create Look Up Table\n",
        "#     input_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\n",
        "#     output_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n",
        "\n",
        "#     # Data Embedding and Converting them into Tensor\n",
        "#     train_inputs, train_outputs = get_tensor_object(df_train, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "#     val_inputs, val_outputs = get_tensor_object(df_val, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "#     test_inputs, test_outputs = get_tensor_object(df_test, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "\n",
        "#     # Transpose column wise\n",
        "#     train_inputs, train_outputs = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\n",
        "#     val_inputs, val_outputs = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\n",
        "#     test_inputs, test_outputs = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n",
        "\n",
        "\n",
        "#     # Initialize Hyperparameters\n",
        "#     input_size = len(input_char_to_int)\n",
        "#     output_size = len(output_char_to_int)\n",
        "#     embedding_size = config.embedding_size\n",
        "#     hidden_size = config.hidden_size\n",
        "#     enc_layers = config.layers\n",
        "#     dec_layers = config.layers\n",
        "#     module_type = config.module_type\n",
        "#     dropout = config.dropout\n",
        "#     learning_rate = config.learning_rate\n",
        "#     batch_size = config.batch_size\n",
        "#     num_epochs = config.num_epochs\n",
        "#     optimizer = config.optimizer\n",
        "#     beam_width = config.beam_search_width\n",
        "#     bidirectional_type = config.bidirectional_type\n",
        "#     length_penalty = config.length_penalty\n",
        "#     teacher_forcing = config.teacher_forcing\n",
        "#     learning_rate = config.learning_rate\n",
        "\n",
        "#     # Create train data batch\n",
        "#     train_batch_x, train_batch_y = torch.split(train_inputs, batch_size, dim=1), torch.split(train_outputs, batch_size, dim=1)\n",
        "#     # Validation data batch\n",
        "#     val_batch_x, val_batch_y = torch.split(val_inputs, batch_size, dim=1), torch.split(val_outputs, batch_size, dim=1)\n",
        "\n",
        "\n",
        "#     # Intialize encoder, decoder and seq2seq model\n",
        "#     encoder = EncoderModule(input_size, embedding_size, hidden_size, enc_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "#     decoder = DecoderModule(output_size, embedding_size, hidden_size, output_size, dec_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "#     model = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, module_type).to(device)\n",
        "\n",
        "#     # Print total number of parameters in the model\n",
        "#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#     print(model)\n",
        "#     print(f'Total Trainable Parameters: {total_params}')\n",
        "\n",
        "\n",
        "#     # Loss function and Optimizer\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     if optimizer == 'adam':\n",
        "#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'sgd':\n",
        "#         optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'rmsprop':\n",
        "#         optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'nadam':\n",
        "#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'adagrad':\n",
        "#         optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "#     else:\n",
        "#         print(\"Incorrect Optmizer !!!!\")\n",
        "\n",
        "#     # TRAINING\n",
        "#     model, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length, 1)\n",
        "#     wandb.log({\n",
        "#             \"accuracy\": acc,\n",
        "#         })\n",
        "\n",
        "# # SWEEP CONFIG\n",
        "# sweep_config = {\n",
        "#     'name': 'sweep_1',\n",
        "#     'method': 'grid',\n",
        "#     'metric': {'name': 'accuracy', 'goal': 'maximize'},\n",
        "#     'parameters': {\n",
        "#         'embedding_size': {'values': [64, 256]},\n",
        "#         'hidden_size': {'values': [256, 512]},\n",
        "#         'layers': {'values': [2, 3]},\n",
        "#         'module_type': {'values':['LSTM', \"GRU\", \"RNN\"]}, # RNN, LSTM, GRU\n",
        "#         'dropout': {'values': [0.3, 0.5]},\n",
        "#         'learning_rate': {'values': [0.01, 0.001]},\n",
        "#         'batch_size': {'values': [32]},\n",
        "#         'num_epochs': {'values': [10]},\n",
        "#         'optimizer': {'values': ['sgd', 'rmsprop', 'adam', 'nadam', 'adagrad']}, # ['sgd', 'rmsprop', 'adam', 'nadam']\n",
        "#         'beam_search_width': {'values': [1, 3, 5]},\n",
        "#         'length_penalty' : {'values': [0.6]},\n",
        "#         'bidirectional_type': {'values': [False, True]},\n",
        "#         'teacher_forcing': {'values': [0.5, 0.7]}\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# # RUN SWEEP ID with agent\n",
        "# sweep_id = wandb.sweep(sweep_config, project = 'DL_Assignment_3')\n",
        "# wandb.agent(sweep_id, main, count = 30)\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41f74d6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-04T03:00:57.246380Z",
          "iopub.status.busy": "2024-05-04T03:00:57.245999Z",
          "iopub.status.idle": "2024-05-04T03:00:57.250575Z",
          "shell.execute_reply": "2024-05-04T03:00:57.249666Z"
        },
        "papermill": {
          "duration": 19.317208,
          "end_time": "2024-05-04T03:00:57.252535",
          "exception": false,
          "start_time": "2024-05-04T03:00:37.935327",
          "status": "completed"
        },
        "tags": [],
        "id": "e41f74d6"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ac9bb0",
      "metadata": {
        "papermill": {
          "duration": 19.410757,
          "end_time": "2024-05-04T03:01:36.470656",
          "exception": false,
          "start_time": "2024-05-04T03:01:17.059899",
          "status": "completed"
        },
        "tags": [],
        "id": "36ac9bb0"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d299620c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-04T03:02:15.574653Z",
          "iopub.status.busy": "2024-05-04T03:02:15.574275Z",
          "iopub.status.idle": "2024-05-04T03:02:15.583366Z",
          "shell.execute_reply": "2024-05-04T03:02:15.582514Z"
        },
        "papermill": {
          "duration": 19.404794,
          "end_time": "2024-05-04T03:02:15.585315",
          "exception": false,
          "start_time": "2024-05-04T03:01:56.180521",
          "status": "completed"
        },
        "tags": [],
        "id": "d299620c"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     parser.add_argument('-dp', '--data_path', type=str, default='kaggle/input/hinid-dataset/aksharantar_sampled/hin', help='Path to the data folder')\n",
        "#     parser.add_argument('-l', '--lang', type=str, default='hin', help='Language for which training is to be done')\n",
        "#     parser.add_argument('-es', '--embedding_size', type=int, default=256, help='Embedding size')\n",
        "#     parser.add_argument('-hs', '--hidden_size', type=int, default=512, help='Hidden size')\n",
        "#     parser.add_argument('-nl', '--layers', type=int, default=2, help='Number of layers')\n",
        "#     parser.add_argument('-ct', '--module_type', type=str, default='LSTM', choices=['RNN', 'LSTM', 'GRU'], help='Cell type (RNN, LSTM, GRU)')\n",
        "#     parser.add_argument('-dr', '--dropout', type=float, default=0.3, help='Dropout rate')\n",
        "#     parser.add_argument('-lr', '--learning_rate', type=float, default=0.01, help='Learning rate')\n",
        "#     parser.add_argument('-bs', '--batch_size', type=int, default=32, help='Batch size')\n",
        "#     parser.add_argument('-ne', '--num_epochs', type=int, default=10, help='Number of epochs')\n",
        "#     parser.add_argument('-op', '--optimizer', type=str, default='adagrad', choices=['adam', 'sgd', 'rmsprop', 'nadam', 'adagrad'], help='Optimizer (adam, sgd, rmsprop, nadam, adagrad)')\n",
        "#     parser.add_argument('-bw', '--beam_search_width', type=int, default=1, help='Beam search width')\n",
        "#     parser.add_argument('-lp', '--length_penalty', type=float, default=0.6, help='Length penalty')\n",
        "#     parser.add_argument('-tf', '--teacher_forcing', type=float, default=0.7, help='Teacher forcing ratio')\n",
        "#     parser.add_argument('-bi', '--bidirectional_type', action='store_true', default=True, help='Use bidirectional_type encoder')\n",
        "#     parser.add_argument('--wandb_log', type=int, default=0, help='Whether to log to WandB (1 for yes, 0 for no)')\n",
        "\n",
        "\n",
        "#     config = parser.parse_args()\n",
        "#     data_path = config.data_path\n",
        "#     lang = config.lang\n",
        "\n",
        "\n",
        "#     # Load Dataset\n",
        "#     df_train, train_input_len, train_out_len = load_dataset(f'/{data_path}/{lang}/{lang}_train.csv')\n",
        "#     df_val, val_input_len, val_out_len = load_dataset(f'/{data_path}/{lang}/{lang}_valid.csv')\n",
        "#     df_test, test_input_len, test_out_len = load_dataset(f'/{data_path}/{lang}/{lang}_test.csv')\n",
        "\n",
        "#     input_max_len = max(train_input_len, val_input_len, test_input_len)\n",
        "#     output_max_len = max(train_out_len, val_out_len, test_out_len)\n",
        "\n",
        "#     max_length = max(input_max_len, output_max_len)\n",
        "\n",
        "#     # Create Look Up Table\n",
        "#     input_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\n",
        "#     output_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n",
        "\n",
        "#     # Data Embedding and Converting them into Tensor\n",
        "#     train_inputs, train_outputs = get_tensor_object(df_train, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "#     val_inputs, val_outputs = get_tensor_object(df_val, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "#     test_inputs, test_outputs = get_tensor_object(df_test, max_length, max_length, input_char_to_int, output_char_to_int)\n",
        "\n",
        "#     # Transpose column wise\n",
        "#     train_inputs, train_outputs = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\n",
        "#     val_inputs, val_outputs = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\n",
        "#     test_inputs, test_outputs = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n",
        "\n",
        "#     # Initialize Hyperparameters\n",
        "#     input_size = len(input_char_to_int)\n",
        "#     output_size = len(output_char_to_int)\n",
        "#     embedding_size = config.embedding_size\n",
        "#     hidden_size = config.hidden_size\n",
        "#     enc_layers = config.layers\n",
        "#     dec_layers = config.layers\n",
        "#     module_type = config.module_type\n",
        "#     dropout = config.dropout\n",
        "#     learning_rate = config.learning_rate\n",
        "#     batch_size = config.batch_size\n",
        "#     num_epochs = config.num_epochs\n",
        "#     optimizer = config.optimizer\n",
        "#     beam_width = config.beam_search_width\n",
        "#     bidirectional_type = config.bidirectional_type\n",
        "#     length_penalty = config.length_penalty\n",
        "#     teacher_forcing = config.teacher_forcing\n",
        "#     learning_rate = config.learning_rate\n",
        "\n",
        "#     # Create train data batch\n",
        "#     train_batch_x, train_batch_y = torch.split(train_inputs, batch_size, dim=1), torch.split(train_outputs, batch_size, dim=1)\n",
        "#     # Validation data batch\n",
        "#     val_batch_x, val_batch_y = torch.split(val_inputs, batch_size, dim=1), torch.split(val_outputs, batch_size, dim=1)\n",
        "\n",
        "\n",
        "#     # Intialize encoder, decoder and seq2seq model\n",
        "#     encoder = EncoderModule(input_size, embedding_size, hidden_size, enc_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "#     decoder = DecoderModule(output_size, embedding_size, hidden_size, output_size, dec_layers, dropout, bidirectional_type, module_type).to(device)\n",
        "#     model = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, module_type).to(device)\n",
        "\n",
        "#     # Print total number of parameters in the model\n",
        "#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#     print(model)\n",
        "#     print(f'Total Trainable Parameters: {total_params}')\n",
        "\n",
        "\n",
        "#     # Loss function and Optimizer\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     if optimizer == 'adam':\n",
        "#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'sgd':\n",
        "#         optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'rmsprop':\n",
        "#         optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'nadam':\n",
        "#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     elif optimizer == 'adagrad':\n",
        "#         optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#     # TRAINING\n",
        "\n",
        "#     if config.wandb_log == 1:\n",
        "#         wandb.init(project='DL_Assignment_3')\n",
        "#         wandb.run.name = 'cell_' + config.module_type + '_bs_' + str(config.batch_size) + '_ep_' + str(config.num_epochs) + '_op_' + str(config.optimizer) + '_drop_' + str(config.dropout) + '_bsw_' + str(config.beam_search_width) +'_emb_' + str(config.embedding_size) + '_hs_' + str(config.hidden_size) + '_elayer_' + str(config.layers) + '_dlayer_' + str(config.layers)\n",
        "\n",
        "#     model, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length, config.wandb_log)\n",
        "#     if config.wandb_log == 1:\n",
        "#         wandb.log({\n",
        "#                 \"accuracy\": acc,\n",
        "#             })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3318a63",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-04T03:02:54.296608Z",
          "iopub.status.busy": "2024-05-04T03:02:54.296232Z",
          "iopub.status.idle": "2024-05-04T03:02:54.300725Z",
          "shell.execute_reply": "2024-05-04T03:02:54.299822Z"
        },
        "papermill": {
          "duration": 19.399916,
          "end_time": "2024-05-04T03:02:54.302652",
          "exception": false,
          "start_time": "2024-05-04T03:02:34.902736",
          "status": "completed"
        },
        "tags": [],
        "id": "c3318a63"
      },
      "outputs": [],
      "source": [
        "# # Example usage\n",
        "# for i in range(10):\n",
        "#     input_seq = df_train.iloc[i, 0][:-1]\n",
        "#     predicted_output = beam_search(model, input_seq, input_char_to_int, output_char_to_int, output_int_to_char, 1, 0.6, \"RNN\")\n",
        "\n",
        "#     print(f\"Input Sequence {i+1}: {input_seq}\")\n",
        "#     print(f\"Predicted Output Sequence {i+1}: {predicted_output}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18596e2e",
      "metadata": {
        "papermill": {
          "duration": 19.717514,
          "end_time": "2024-05-04T03:03:33.635967",
          "exception": false,
          "start_time": "2024-05-04T03:03:13.918453",
          "status": "completed"
        },
        "tags": [],
        "id": "18596e2e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb19ba0d",
      "metadata": {
        "papermill": {
          "duration": 19.274108,
          "end_time": "2024-05-04T03:04:12.450756",
          "exception": false,
          "start_time": "2024-05-04T03:03:53.176648",
          "status": "completed"
        },
        "tags": [],
        "id": "bb19ba0d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "126c0e23",
      "metadata": {
        "papermill": {
          "duration": 19.667379,
          "end_time": "2024-05-04T03:04:51.549853",
          "exception": false,
          "start_time": "2024-05-04T03:04:31.882474",
          "status": "completed"
        },
        "tags": [],
        "id": "126c0e23"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 883208,
          "sourceId": 1508471,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4915921,
          "sourceId": 8278287,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 27738.475675,
      "end_time": "2024-05-04T03:05:13.889617",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-05-03T19:22:55.413942",
      "version": "2.5.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}