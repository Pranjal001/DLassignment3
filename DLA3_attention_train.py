# -*- coding: utf-8 -*-
"""DLA_A3_VANILLA_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VAkz01syRysnEN5v0k3QqIDuDtZ1ENxN
"""

import torch
import pandas as pd
import copy
import numpy as np
from torch.autograd import Variable
from tqdm import tqdm
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import random
import heapq
import torch.optim as optim
import torch.nn as nn
import argparse
import wandb
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""# Preprocessing"""
#Language class preprocess the train test and validation data for training and evaluation
class lang:
    def __init__(self,path_train,path_val,path_test):
        self.path_train = path_train
        self.path_val = path_val
        self.path_test = path_test
        self.trainfile = pd.read_csv(path_train,header=None, encoding='utf-8')
        self.valfile = pd.read_csv(path_val,header=None, encoding='utf-8')
        self.testfile = pd.read_csv(path_test,header=None, encoding='utf-8')

    def datasetencoder(self,file):

        # Update file[0]
        file[0] = [x + '>' for x in file[0]]

        # Update file[1]
        file[1] = ['<' + x + '>' for x in file[1]]

        # Calculate maximum length of unique elements in file[0]
        ipmax = 0
        for x in file[0].unique():
            if len(x) > ipmax:
                ipmax = len(x)

        # Calculate maximum length of unique elements in file[1]
        opmax = 0
        for x in file[1].unique():
            if len(x) > opmax:
                opmax = len(x)


        return ipmax,opmax,file

    def dictionary_create(self,data):
        #creates a dictionary which helps in converting char to int and int to char
        data.discard('<')
        data.discard('>')
        chartoint = {"": 0, '<':1, '>':2}
        inttochar = {}

        for ci, c in enumerate(sorted(data), len(chartoint)):
            chartoint[c] = ci
        for c, ci in chartoint.items():
            inttochar[ci] = c

        return chartoint,inttochar

    def convert_tensor_element(self,data , length , chartoint):
        #this functioin converts the given input element to a tensor element
        data_enc = np.zeros(length)
        encoder = []
        for char in data:
            encoder.append(chartoint[char])
        encoder = np.array(encoder)

        if len(encoder) < length:
          length = len(encoder)

        data_enc[:length] = encoder[:length]

        return torch.tensor(data_enc, dtype=torch.int64)

    def convert_tensor_data(self,data,maxlength_ip, chartoint_ip,maxlength_op, chartoint_op):
        #this functioin converts the given input data to a tensor data
        tensor_obj_input = []
        tensor_obj_output = []

        for ip, op in zip(data[0], data[1]):
            # Encode input string
            temp_input = self.convert_tensor_element(ip, maxlength_ip, chartoint_ip)
            tensor_obj_input.append(temp_input)

            # Encode output string
            temp_output = self.convert_tensor_element(op, maxlength_op, chartoint_op)
            tensor_obj_output.append(temp_output)

        tensor_obj_input =  torch.stack(tensor_obj_input)
        tensor_obj_output = torch.stack(tensor_obj_output)


        return tensor_obj_input , tensor_obj_output

    def preparedata(self):
        #this function cummulates all the operations on the data and returns the processed data for training and evaluation

        train_ipmax , train_opmax , train = self.datasetencoder(self.trainfile)
        val_ipmax , val_opmax , val =self.datasetencoder(self.valfile)
        test_ipmax , test_opmax , test =self.datasetencoder(self.testfile)

        input_char_to_int,input_int_to_char  = self.dictionary_create(set(''.join(train[0]) + ''.join(val[0]) + ''.join(test[0])))
        output_char_to_int ,output_int_to_char= self.dictionary_create(set(''.join(train[1]) + ''.join(val[1]) + ''.join(test[1])))

        ipmax = max(train_ipmax ,val_ipmax ,test_ipmax)
        opmax = max(train_opmax ,val_opmax , test_opmax)

        train_tensor_ip, train_tensor_op = self.convert_tensor_data(train,ipmax,input_char_to_int,opmax,output_char_to_int)
        val_tensor_ip, val_tensor_op = self.convert_tensor_data(val,ipmax,input_char_to_int,opmax,output_char_to_int)
        test_tensor_ip, test_tensor_op = self.convert_tensor_data(test,ipmax,input_char_to_int,opmax,output_char_to_int)

        #transpose data tensor

        train_tensor_ip, train_tensor_op = train_tensor_ip.t(),train_tensor_op.t()
        val_tensor_ip, val_tensor_op = val_tensor_ip.t(), val_tensor_op.t()
        test_tensor_ip, test_tensor_op = test_tensor_ip.t() , test_tensor_op.t()

        len_max = max(ipmax,opmax)

        return train_tensor_ip, train_tensor_op ,val_tensor_ip, val_tensor_op,test_tensor_ip, test_tensor_op,input_char_to_int,input_int_to_char,output_char_to_int ,output_int_to_char,val,test,len_max

        #do all func call in this and return final datasets



"""## Modules for encoder and decoder"""
#ENCODER MODULE STARTS HERE

class EncoderModule(nn.Module):
    def __init__(self, input_size, embedding_size, hidden_size, layers, dropout, bidirectional, module_type):
        #init class to assign initial values to the encoder object

        super(EncoderModule, self).__init__()
        self.mng = -1
        self.hidden_size = hidden_size
        self.bidirectional = bidirectional
        self.dropout = nn.Dropout(dropout)
        self.count = 0
        self.module_dict = ["GRU" , "LSTM" , "RNN"]
        self.embedding = nn.Embedding(input_size, embedding_size)
        self.layers = layers
        self.module_type = module_type




        if module_type == self.module_dict[0]:
            self.rnn = nn.GRU(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)
        if module_type == self.module_dict[2]:
            self.rnn = nn.RNN(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)
        if module_type == self.module_dict[1]:
            self.rnn = nn.LSTM(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)

    #FUNCTION for forward propagation for encoder module
    def forward(self, x):
        embedding = self.embedding(x)
        embedding = self.dropout(embedding)

        if self.module_type == "GRU" :

            outputs, hidden = self.rnn(embedding) # outputs shape: (seq_length, N, hidden_size)
            if self.bidirectional == True:

                outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]
                hidden = torch.cat((hidden[: self.layers], hidden[self.layers:]), dim=0)

            # Return hidden state and cell state
            return hidden

        if self.module_type == "LSTM":

            outputs, (hidden, cell) = self.rnn(embedding)
            if self.bidirectional == True:
                hidden = torch.cat((hidden[: self.layers], hidden[self.layers:]), dim=0)
                outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]
            return hidden, cell

        if self.module_type == "RNN":
            # Pass through the RNN/GRU layer
            outputs, hidden = self.rnn(embedding) # outputs shape: (seq_length, N, hidden_size)
            if self.bidirectional == True:
                # Sum the bidirectional outputs
                outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]
                hidden = torch.cat((hidden[: self.layers], hidden[self.layers:]), dim=0)

            # Return hidden state and cell state
            return hidden

#DECODER MODULE STARTS HERE

class DecoderModule(nn.Module):
    def __init__(self, input_size, embedding_size, hidden_size, output_size, layers, dropout, bidirectional, module_type):
        #init class to assign initial values to the encoder object
        #also construct the appropriate layers for the model
        super(DecoderModule, self).__init__()
        self.mng = -1
        self.bidirectional = bidirectional
        self.count = 0
        self.module_dict = ["GRU" , "LSTM" , "RNN"]
        self.layers = layers
        self.embedding_size = embedding_size
        self.dropout = nn.Dropout(dropout)
        self.hidden_size = hidden_size
        self.module_type = module_type
        self.embedding = nn.Embedding(input_size, embedding_size)

        # Define RNN layer with specific cell type

        if module_type == self.module_dict[0]:
            self.rnn = nn.GRU(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)
        if module_type == self.module_dict[1]:
            self.rnn = nn.LSTM(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)
        if module_type == self.module_dict[2]:
            self.rnn = nn.RNN(embedding_size, hidden_size, layers, dropout=dropout, bidirectional=bidirectional)

        #modifying input size based on bidirectional type

        if bidirectional:
          input_size = hidden_size * 2
        else:
          input_size = hidden_size

        #fully connected layera and output function that is softmax

        self.fc = nn.Linear(input_size, output_size)
        self.log_softmax = nn.LogSoftmax(dim=1)


    def forward(self, x, hidden, cell):
        #forward propagation function for Decoder module 

        embedding = self.embedding(x.unsqueeze(0))
        embedding = self.dropout(embedding)


        if self.module_type == "GRU":

            outputs, hidden = self.rnn(embedding, hidden)
            out = self.fc(outputs)
            out = out.squeeze(0)
            predictions = self.log_softmax(out)
            return predictions, hidden

        if self.module_type == "RNN":

            outputs, hidden = self.rnn(embedding, hidden)
            out = self.fc(outputs)
            out = out.squeeze(0)
            predictions = self.log_softmax(out)
            return predictions, hidden

        if self.module_type == "LSTM":

            outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))
            out = self.fc(outputs)
            out = out.squeeze(0)
            predictions = self.log_softmax(out)
            return predictions, hidden, cell

"""## Seq2Seq Class"""
#sequence to sequence MODULE STARTS HERE
class Seq2Seq(nn.Module):

    def __init__(self, encoder, decoder, output_char_to_int, teacher_forcing, module_type):

        super(Seq2Seq, self).__init__()
        #init class to assign initial values to the S2S object
        self.encoder = encoder
        self.decoder = decoder
        self.mng = -1
        self.counttemp = 0
        self.teacher_force_ratio = teacher_forcing
        self.module_type = module_type
        self.target_vocab_size = len(output_char_to_int)

    def forward(self, source, target):
        #forward propagation for S2S module
        target_vocab_size = self.target_vocab_size
        teacher_force_ratio = self.teacher_force_ratio
        batch_size = source.shape[1]


        outputs = torch.zeros(target.shape[0], batch_size, target_vocab_size).to(source.device)

        x = target[0]

        if self.module_type == 'LSTM':
            hidden, cell = self.encoder(source)
        if self.module_type == 'GRU':
            hidden = self.encoder(source)
        if self.module_type == 'RNN':
            hidden = self.encoder(source)


        for t in range(1, target.shape[0]):

            if self.module_type == 'LSTM':
                output, hidden, cell = self.decoder(x, hidden, cell)
            if self.module_type == 'GRU':
                output, hidden = self.decoder(x, hidden, None)
            if self.module_type == 'RNN':
                output, hidden = self.decoder(x, hidden, None)


            outputs[t] = output
            best_guess = output.argmax(1)
            if random.random() >= teacher_force_ratio:
              x = best_guess
            else:
              x = target[t]


        return outputs

"""# TRAINING"""
# Beam search function for training
def beam_search(model, input_seq, max_length, input_char_index, output_char_index, reverse_target_char_index, beam_width, length_penalty, cell_type):

    if len(input_seq) > max_length:
        return ""

    #initializing input_data with np zeros
    input_data = np.zeros((max_length, 1), dtype=int)
    count = 0

    for idx, char in enumerate(input_seq):
        input_data[idx, 0] = input_char_index[char]
        count += 1
    input_data[idx + 1, 0] = input_char_index[">"]

    if(count < 0 ):
      input_tensor = torch.tensor(input_data, dtype=torch.int64).to(device)
    else :
      input_tensor = torch.tensor(input_data, dtype=torch.int64).to(device)

    with torch.no_grad():

        if cell_type == 'GRU':
            hidden = model.encoder(input_tensor)

        if cell_type == 'RNN':
            hidden = model.encoder(input_tensor)

        if cell_type == 'LSTM':
            hidden, cell = model.encoder(input_tensor)

    out_t = output_char_index['<']
    out_reshape = np.array(out_t).reshape(1,)
    initial_sequence = torch.tensor(out_reshape).to(device)
    hidden_par = hidden.unsqueeze(0)
    beam = [(0.0, initial_sequence, hidden_par)]

    lcn = len(output_char_index)
    for _ in range(lcn):
        candidates = []
        for score, seq, hidden in beam:
            if output_char_index['>'] == seq[-1].item():

                counttem = 0
                candidates.append((score, seq, hidden))
                continue

            last_token = np.array(seq[-1].item())
            last_token = last_token.reshape(1,)
            x = torch.tensor(last_token).to(device)

            if cell_type == 'GRU':
                output, hidden,  = model.decoder(x, hidden.squeeze(0), None)
            if cell_type == 'RNN':
                output, hidden,  = model.decoder(x, hidden.squeeze(0), None)
            if cell_type == 'LSTM':
                output, hidden, cell,  = model.decoder(x, hidden.squeeze(0), cell)


            probabilities = F.softmax(output, dim=1)
            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)

            itrtopk = topk_probs[0]
            itrtoktokens =  topk_tokens[0]
            for prob, token in zip(itrtopk, itrtoktokens):
                tokentemp = token.unsqueeze(0)
                new_seq = torch.cat((seq, tokentemp), dim=0)
                length_newsq = len(new_seq)
                seq_length_norm_factor = (length_newsq  - 1)
                seq_length_norm_factor = seq_length_norm_factor/5
                candidate_score_deno =  (seq_length_norm_factor ** length_penalty)
                candidate_score = score + torch.log(prob).item() / candidate_score_deno
                candidates.append((candidate_score, new_seq, hidden.unsqueeze(0)))

        keyq = lambda x: x[0]
        #finds the n largest among all possible candidates 
        beam = heapq.nlargest(beam_width, candidates, key = keyq)

    best_score = float('-inf')
    best_sequence = None

    for score, sequence, _ in beam:
        if score > best_score:
            best_score = score
            best_sequence = sequence

    result = []
    for token in best_sequence[1:]:
        char = reverse_target_char_index[token.item()]
        result.append(char)

    final_string = ''.join(result)

    return final_string




# TRAINING FUNCTION
def train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length, wandb_log):

    for epoch in range(num_epochs):
        total_loss = 0
        total_words = 0
        accuracy = 0
        correct_pred = 0
        model.train()

        #learning the parameters with backward propagation 
        for (x, y) in tqdm(zip(train_batch_x, train_batch_y), total=len(train_batch_x)):



            optimizer.zero_grad()
            output = model(x.to(device), y.to(device))

            target = y.to(device)
            target = target.reshape(-1)
            output = output.reshape(-1, output.shape[2])
            loss = criterion(output[(target != 0)], target[(target != 0)])


            loss.backward()


            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
            optimizer.step()

            total_loss = total_loss + loss.item()

            total_words = total_words + target.size(0)
            checkq = torch.argmax(output, dim=1) == target
            correct_pred = correct_pred + torch.sum(checkq).item()

        avg_loss = total_loss / len(train_batch_x)
        acc = correct_pred / total_words
        acc *= 100

        model.eval()

        #evaluating the model over train data

        with torch.no_grad():
            val_total_loss = 0
            val_total_words = 0
            val_correct_pred = 0


            for x_val, y_val in tqdm(zip(val_batch_x, val_batch_y),total =len(val_batch_x)):
                target_val = y_val.to(device)
                inp_data_val = x_val.to(device)

                output_val = model(inp_data_val, target_val)


                target_val = target_val.reshape(-1)
                opvalshape = output_val.shape[2]
                output_val = output_val.reshape(-1, opvalshape)

                pad_mask = (target_val != 0)
                target_val = target_val[pad_mask]
                output_val = output_val[pad_mask]

                val_loss = criterion(output_val, target_val)
                val_total_loss = val_total_loss+ val_loss.item()
                val_total_words = val_total_words+ target_val.size(0)
                checkq = torch.argmax(output_val, dim=1) == target_val
                val_correct_pred = val_correct_pred+ torch.sum(checkq).item()


            val_acc = val_correct_pred / val_total_words
            val_acc = 100*val_acc
            val_avg_loss = val_total_loss / len(val_batch_x)

        #evaluating the model over val data
        beam_val_pred = 0
        beam_val = 0
        itrtqdm = tqdm(range(df_val.shape[0]))
        for i in itrtqdm:
            input_seq = df_val.iloc[i, 0][:-1]
            true_seq = df_val.iloc[i, 1][1:-1]
            predicted_output = beam_search(model, input_seq, max_length, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type)
            cod = true_seq == predicted_output[:-1]
            if cod:
                beam_val_pred = beam_val_pred + 1
        beam_val = beam_val_pred/df_val.shape[0]
        beam_val = 100*beam_val
        #evaluating the word accuracy for validation data

        #printing results after each epochs
        print("========================================================================")
        print(f"---------------------------- Epoch : ",epoch+1,"------------------------")
        print(f"Train accuracy Character: ",acc)
        print(f"Train Average Loss: ",avg_loss)
        print(f"Validation accuracy Character: ",val_acc)
        print(f"Validation Average Loss: ",val_avg_loss)
        print(f"Beam Val Word accuracy: " ,beam_val)
        print(f"Correct Prediction : {beam_val_pred}/{df_val.shape[0]}")
        print("========================================================================")
        if wandb_log == 1:
            wandb.log({
                "train_accuracy_char": acc,
                "train_loss": avg_loss,
                "val_accuracy_char": val_acc,
                "val_loss": val_avg_loss,
                "beam_val_accuracy_word" : beam_val,
            })

    return model, beam_val


#function to assign relevent optimizer for the model
def assign_opt(optimizer):
  if optimizer == 'adam':
      return optim.Adam(model.parameters(), lr=learning_rate)
  if optimizer == 'sgd':
      return optim.SGD(model.parameters(), lr=learning_rate)
  if optimizer == 'rmsprop':
      return optim.RMSprop(model.parameters(), lr=learning_rate)
  if optimizer == 'nadam':
      return optim.Adam(model.parameters(), lr=learning_rate)
  if optimizer == 'adagrad':
      return optim.Adagrad(model.parameters(), lr=learning_rate)


#argparse for command line arguments
parser = argparse.ArgumentParser()
parser.add_argument('-dp', '--data_path', type=str, default='kaggle/input/hinid-dataset/aksharantar_sampled/hin', help='Path to the data folder')
parser.add_argument('-ln', '--lng', type=str, default='hin', help=' provide ehich language you want it to be trained on')
parser.add_argument('-es', '--embedding_size', type=int, default=256, help='Embedding size')
parser.add_argument('-hs', '--hidden_size', type=int, default=512, help='Hidden size')
parser.add_argument('-l', '--layers', type=int, default=3, help='layers size')
parser.add_argument('-mt', '--module_type', type=str, default='GRU', choices=['RNN', 'LSTM', 'GRU'], help='Module type (RNN, LSTM, GRU)')
parser.add_argument('-dt', '--dropout', type=float, default=0.3, help='Dropout rate')
parser.add_argument('-lr', '--learning_rate', type=float, default=0.0001, help='Learning rate')
parser.add_argument('-bs', '--batch_size', type=int, default=32, help='Batch size')
parser.add_argument('-e', '--num_epochs', type=int, default=10, help='Number of epochs')
parser.add_argument('-opt', '--optimizer', type=str, default='nadam', choices=['adam', 'sgd', 'rmsprop', 'nadam', 'adagrad'], help='Optimizer (adam, sgd, rmsprop, nadam, adagrad)')
parser.add_argument('-bw', '--beam_search_width', type=int, default=1, help='Beam search width')
parser.add_argument('-lp', '--length_penalty', type=float, default=0.4, help='Length penalty')
parser.add_argument('-tf', '--teacher_forcing', type=float, default=0.5, help='Teacher forcing ratio')
parser.add_argument('-bi', '--bidirectional_type', default=True, help='Use bidirectional_type encoder')
parser.add_argument('-wl', '--wandb_log', type=int, default = 0, help='Whether to log to WandB (1 for yes, 0 for no)', choices=[0, 1])
parser.add_argument('-wp', '--wandb_project',help='Project name used to track experiments in Weights & Biases dashboard', type=str, default='DL_assignment_3')
parser.add_argument('-we', '--wandb_entity', help='Wandb Entity used to track experiments in the Weights & Biases dashboard.', type=str, default='cs23m049')



config = parser.parse_args()
if config.wandb_log == 1:
    run_config = vars(config).copy()
    run_config.pop('data_path', None)
    run_config.pop('lang', None)
    run_config.pop('wandb_log', None)
    run_config.pop('wandb_project', None)
    run_config.pop('wandb_entity', None)
    run_config.pop('heatmap_plot', None)
    wandb.init(config=run_config, project=config.wandb_project, name=config.wandb_entity)
    wandb.run.name = 'attention->'+':cell:' + config.module_type + ':bs:' + str(config.batch_size) + ':ep:' + str(config.num_epochs) + ':op:' + str(config.optimizer) + ':drop:' + str(config.dropout) + ':bsw:' + str(config.beam_search_width) +':emb:' + str(config.embedding_size) + ':hs:' + str(config.hidden_size) + ':elayer:' + str(config.layers) + ':dlayer:' + str(config.layers)



#assigning data from the language class and futther trianing and evaluating the model 

pathtrain = config.data_path+"/"+config.lng+"/"+config.lng+"_train.csv"
pathval =config.data_path+"/"+config.lng+"/"+config.lng+"_valid.csv"
pathtest = config.data_path+"/"+config.lng+"/"+config.lng+"_test.csv"

language = lang(pathtrain , pathval,pathtest)
train_ip, train_op ,val_ip, val_op,test_ip, test_op,input_char_to_int,input_int_to_char,output_char_to_int ,output_int_to_char,df_val,df_test,max_length = language.preparedata()

# Initialize Hyperparameters
# input_size = len(input_char_to_int)
# output_size = len(output_char_to_int)
# embedding_size = 64
# hidden_size = 256
# enc_layers = 2
# dec_layers = 2
# module_type = "LSTM"
# dropout = 0.3
# learning_rate = 0.1
# batch_size = 64
# num_epochs = 1
# optimizer = "adagrad"
# beam_width = 1
# bidirectional_type = True
# length_penalty = 0.6
# teacher_forcing = 0.5


input_size = len(input_char_to_int)
output_size = len(output_char_to_int)
embedding_size = config.embedding_size
hidden_size = config.hidden_size
enc_layers = config.layers
dec_layers = config.layers
module_type = config.module_type
dropout = config.dropout
learning_rate = config.learning_rate
batch_size = config.batch_size
num_epochs = config.num_epochs
optimizer = config.optimizer
beam_width = config.beam_search_width
bidirectional_type = config.bidirectional_type
length_penalty = config.length_penalty
teacher_forcing = config.teacher_forcing

# Create train data batch
train_batch_x = torch.split(train_ip, batch_size, dim=1)
train_batch_y =  torch.split(train_op, batch_size, dim=1)
# Create val data batch
val_batch_x = torch.split(val_ip, batch_size, dim=1)
val_batch_y =  torch.split(val_op, batch_size, dim=1)


# Intialize encoder, decoder and seq2seq model
encoder = EncoderModule(input_size, embedding_size, hidden_size, enc_layers, dropout, bidirectional_type, module_type).to(device)
decoder = DecoderModule(output_size, embedding_size, hidden_size, output_size, dec_layers, dropout, bidirectional_type, module_type).to(device)
model = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, module_type).to(device)

#prints total parameters and the model structure
total_params = 0
for p in model.parameters():
    if p.requires_grad:
        total_params += p.numel()

print(model)
print(f'Total Trainable Parameters: {total_params}')


# Loss function and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = assign_opt(optimizer)


# TRAINING
model, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, module_type, max_length, config.wandb_log)

if config.wandb_log == 1:
    wandb.log({
            "accuracy": acc,
        })

test_acc = 0
correct_pred = 0
words_test = []
translations_test = []
predictions_test = []
results_test = []

for i in tqdm(range(df_test.shape[0])):
    input_seq = df_test.iloc[i, 0][:-1]
    true_seq = df_test.iloc[i, 1][1:-1]
    predicted_output = beam_search(model, input_seq, max_length, input_char_to_int, output_char_to_int,
                                   output_int_to_char, beam_width, length_penalty, module_type)
    words_test.append(input_seq)
    translations_test.append(true_seq)
    predictions_test.append(predicted_output[:-1])
    if true_seq == predicted_output[:-1]:
        correct_pred += 1
        results_test.append('Yes')
    else:
        results_test.append('No')

test_acc = 100 * correct_pred / df_test.shape[0]


#print the models final test accuracy 
print(f'Test Accuracy Word Level: {test_acc}, Correctly Predicted: {correct_pred}')
print("================================================================")

